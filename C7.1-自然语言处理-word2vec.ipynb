{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chap 7：自然语言处理**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 11:09:06.271779: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-31 11:09:06.458791: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-31 11:09:06.491476: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-05-31 11:09:07.619773: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/chenguangze/software/miniconda3/lib/:/home/chenguangze/software/miniconda3/lib/:/home/chenguangze/software/miniconda3/envs/tensorflow/lib/\n",
      "2023-05-31 11:09:07.619922: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/chenguangze/software/miniconda3/lib/:/home/chenguangze/software/miniconda3/lib/:/home/chenguangze/software/miniconda3/envs/tensorflow/lib/\n",
      "2023-05-31 11:09:07.619930: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from source.code import ch7\n",
    "from source.code import utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果你要在服务器上和别人共用GPU，可以设置你需要的显存资源\n",
    "utils.gpu_limitation_config(memory=30,device=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7.1 词嵌入 word2vec**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7.1.1 自监督的词向量表示 word2vec**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1) 词向量表示的背景**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 `RNN` 部分的学习过程中，我们介绍了部分自然语言处理的相关知识，现在我们将进一步深入讨论"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对文本这种序列进行建模时，我们通常需要先将文本按照某种规则**进行分词**（`tokenize`），然后**创建词表**（`Vocab`），词表负责将分词后的每个字符串映射到一个**整数索引**，但在输入给模型之前，我们不会直接使用整数索引，因为**索引变换范围很大**（`[0, vocab_size]`），这会带来极大的**数值不稳定性**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一种处理这种离散编码的方案是**One Hot编码**（虽然我们在 `RNN` 一节中没有使用），它将索引为 $i$ 的变量编码到一个长度为 `vocab_size` 的 **01 二值向量**，使其第 $i$ 个分量为 1，其余分量为 0\n",
    "* 独热编码虽然思想简单，操作容易，但通常并不是一个合适的选择\n",
    "* 一方面，它会带来**稀疏矩阵**的问题，浪费不必要的内存空间\n",
    "* 另一方面，词元的独热编码表示依然没办法满足**相近含义的词应该有相近的词向量表示**的要求"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例如考虑两个词向量 $x,y\\in\\mathbb{R}^{p}$，计算它们的**余弦相似度**：\n",
    "$$\n",
    "\\frac{x^Ty}{\\|x\\| \\|y\\|} \\in [-1,1]\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们希望含义相近的词具有越高的预先相似度，但**对于独热编码而言，任意两个词的余弦相似度都等于 0**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2) word2vec 模型**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，人们提出**嵌入层**（`Embedding Layer`）的概念，嵌入层负责将每个词元映射到一个固定维度 $p$ 的**稠密向量**，并希望该稠密向量空间能够正确表示词元的含义，`word2vec` 就是词嵌入重要的工作之一，word2vec 包含两种不同的建模思路：**跳元模型**（**Skip-Gram**），和**连续词袋**（**CBOW**）\n",
    "* 这两种模型都从语义的角度对文本序列建模，训练依赖于条件概率（即用文本序列中的一些词去预测另一些词）\n",
    "* 由于这些文本序列建模过程中，**不需要额外标注的标签信息**，文本序列建模的一大特点是可以从文本自身出发构造出标签，因此这类模型称为**自监督模型**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **跳元模型 Skip-Gram**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "跳元模型假设**一个词可以用来在文本序列中生成其周围的词**，假设词元序列为 $x_1,x_2,\\cdots,x_T$，中心词是 $x_k$，并**将上下文窗口大小设置为** $2$，则建模考虑的条件概率为：\n",
    "$$\n",
    "P(x_{k-2},x_{k-1},x_{k+1},x_{k+2}|x_k)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果**假设上下文词在给定中心词时是独立生成的**，则条件概率由独立性假设会变得更加简单：\n",
    "$$\n",
    "P(x_{k-2}|x_k)\\cdot P(x_{k-1}|x_k)\\cdot P(x_{k+1}|x_k)\\cdot P(x_{k+2}|x_k)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在跳元模型中，每个词元 $x_i$ 会**有两个维度为 $p$ 的词向量表示** $v_i,u_i\\in\\mathbb{R}^p$，它们分别作为**中心词表示**和**上下文表示**：\n",
    "* 当把 $x_i$ **作为中心词使用时**，它将用 $v_i$ 来表示其嵌入结果\n",
    "* 而如果 $x_i$ 被**作为上下文词使用时**，它将用 $u_i$ 来表示其嵌入结果"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，当给定中心词 $w_c$（假设在词表中的索引为 $c$），其生成任何上下文词元 $w_o$（假设在词表中的索引为 $o$）的**条件概率被建模为**：\n",
    "$$\n",
    "P(w_o|w_c) = \\frac{ \\exp(u_o^Tv_c) }{ \\sum_{i\\in\\mathcal{V}} \\exp(u_i^Tv_c) }\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，$\\mathcal{V}$ 是词表中词元索引集合，当给定一个长度为 $T$ 的文本序列，$w_t$ 表示时间步 $t$ 处的词元，并**继续使用条件概率的独立性假设**，则对于上下文窗口 $m$，该序列的**跳元模型的似然概率可以写作**：\n",
    "$$\n",
    "\\prod_{t=1}^{T} \\prod_{\\substack{-m\\leq j\\leq m,j\\ne 0\\\\ 1\\leq t+j \\leq T}} P(w_{t+j} | w_t)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在建模时，可以省略所有时间步小于 $1$ 和大于 $T$ 的概率，训练时，**中心词向量和上下文词向量** $v_i,u_i$ **作为模型参数进行更新**，通过**最大化似然函数**或者**最小化下方的对数似然函数**来驱动模型训练，它就是模型的**损失函数** $L$：\n",
    "$$\n",
    "L = -\\sum_{t=1}^{T} \\sum_{-m\\leq j\\leq m,j\\ne 0} \\log P(w_{t+j}|w_t)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练结束后，我们得到每个词元 $x_i$ 的中心词表示 $v_i\\in\\mathbb{R}^p$，和上下文词表示 $u_i\\in\\mathbb{R}^p$，**在自然语言处理应用中，跳元模型的中心词向量通常用作词嵌入的表示**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **连续词袋模型 CBOW**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "连续词袋模型类似于跳元模型，只不过**将建模方向调换一下**，我们**用上下文词元来预测中心词元**，同样给定序列 $x_1,x_2,\\cdots,x_T$，中心词是 $x_k$，并将**上下文窗口长度设置为 2**，则建模考虑的条件概率为：\n",
    "$$\n",
    "P(x_k|x_{k-2},x_{k-1},x_{k+1},x_{k+2})\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于 CBOW 模型中存在多个上下文词元，因此在计算条件概率时需要**对这些上下文词向量进行平均**，我们用 $v_i,u_i\\in\\mathbb{R}^p$ 是**词元的上下文词向量表示和中心词向量表示**（**注意，这里与跳元模型中的符号相反**），给定上下文序列 $w_{o_1},\\cdots,w_{o_{2m}}$，下标表示词元索引，它们生成中心词 $w_c$，则条件概率为：\n",
    "$$\n",
    "P(w_c | w_{o_1},\\cdots,w_{o_{2m}}) = \\frac{ \\exp{\\left( \\frac{1}{2m} u_c^T(v_{o_1} + \\cdots + v_{o_{2m}}) \\right)} }{ \\sum_{i\\in\\mathbb{V}} \\exp{\\left( \\frac{1}{2m} u_i^T(v_{o_1} + \\cdots + v_{o_{2m}}) \\right)}  }\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们记 $W_o = \\{w_{o_1},\\cdots,w_{o_{2m}}\\}$，以及上下文的均值向量 $\\overline{v}_o = (v_{o_1} + \\cdots + v_{o_{2m}}) / (2m)$，则上式可以简化：\n",
    "$$\n",
    "P(w_c | W_o) = \\frac{ \\exp(u_c^T \\overline{v}_o) }{ \\sum_{i\\in\\mathbb{V} } \\exp(u_i^T \\overline{v}_o) }\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当给定一个长度为 $T$ 的文本序列，则对于上下文窗口 $m$，该序列的**连续词袋模型的似然概率可以写作**：\n",
    "$$\n",
    "\\prod_{t=1}^{T} P(w_t | w_{t-m},\\cdots,w_{t-1},w_{t+1},\\cdots,w_{t+m})\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练模型参数 $v_i,u_i$ 时，依然是通过**最小化负对数似然进行的**：\n",
    "$$\n",
    "L = -\\sum_{t=1}^T \\log P(w_t|w_{t-m},\\cdots,w_{t-1},w_{t+1},\\cdots,w_{t+m})\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练结束后，我们得到每个词元 $x_i$ 的上下文表示 $v_i\\in\\mathbb{R}^p$，和中心词表示 $u_i\\in\\mathbb{R}^p$，**与跳元模型不同，CBOW 的上下文词向量用作词嵌入的表示**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7.1.2 word2vec 模型预训练**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于跳元模型和连续词袋模型的相似性，后文中，我们仅以跳元模型为例进行说明"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1) 近似训练**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "无论是跳元模型还是连续词袋模型，在实际应用中，词表大小 $|\\mathcal{V}|$ 可能达到几十万，概率建模中包含的 `softmax` 操作都需要计算 $|\\mathcal{V}|$ 个求和项，再考虑到模型更新时梯度下降，**求和的梯度计算成本巨大**，为了降低计算复杂度，两种主流的处理思路是**负采样**和**分层 softmax**\n",
    "* **负采样**\n",
    "  * 定义**事件** $S$：给定中心词 $w_c$ 和上下文窗口，任何上下文词 $w_o$ 来自该上下文窗口，该事件的概率用下式建模：\n",
    "    $$\n",
    "    P(D=1|w_c,w_o) = \\sigma(u_o^Tv_c)\n",
    "    $$\n",
    "    $\\sigma$ 是 `sigmoid` 函数，将内积映射为概率\n",
    "  * 考虑从**最大化序列中所有上述事件的联合概率**的角度出发，则目标变为最大化下面的概率模型：\n",
    "    $$\n",
    "    \\prod_{t=1}^{T}\\prod_{-m\\leq j\\leq m,j\\ne 0} P(D=1|w_t,t_{t+j})\n",
    "    $$\n",
    "    但上式只考虑了**正样本事件**（上述概率仅当所有词向量趋于无穷时，概率取得最大值 1，这**显然毫无意义**），因此，为了使目标函数变得有意义，**负采样技术**从预定义的分布中采样负样本，并添加到概率模型中\n",
    "  * 从预定义的分布 $P(w)$ 中采样 $K$ 与事件 $S$ 相悖的**噪声词**，并用 $N_k$ 表示噪声词 $w_k,k=1,\\cdots,K$ 不是来自 $w_c$ 的上下文窗口的事件，假设正样本和负样本 $S,N_1,\\cdots,N_K$ **事件相互独立**，则负采样会将上述仅涉及正样本的联合概率改写为：\n",
    "    $$\n",
    "    \\prod_{t=1}^{T}\\prod_{-m\\leq j\\leq m,j\\ne 0} P(w_{t+j} | w_t)\n",
    "    $$ \n",
    "    通过事件 $S,N_1,\\cdots,N_K$ 的**近似条件概率** 对 $P(w_{t+j}| w_t)$ 建模：\n",
    "    $$\n",
    "    P(w_{t+j}|w_t) = P(D=1| w_t,w_{t+j}) \\prod_{k=1, w_k\\sim P(w)}^{K} P(D=0|w_t,w_k)\n",
    "    $$\n",
    "    其中，概率 $P(D=0|w_t,w_k) = 1 - P(D=1|w_t,w_k)$\n",
    "  * 现在经过转换，每个训练步中，模型推理，梯度计算的成本都与词表大小 $|\\mathcal{V}|$ 无关了，而是**线性依赖于负采样个数** $K$，当将超参数 $K$ 设置为较小的数时，负采样每个训练步的梯度计算成本较小"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **分层 softmax / 层序 softmax**\n",
    "  * 虽然分层 softmax 名字中依然包含 softmax，但实际的计算过程与 softmax 没有关系，算法的第一步，需要**根据每个词元的词频，创建一棵 Huffman 二叉树**，**越靠近树根，词元词频越高**，最终，**每个词元会被编码到二叉树的某个叶子结点上**（具体可以查阅 [Huffman二叉树](https://blog.csdn.net/qq_46785243/article/details/127622507)）\n",
    "  * 假设下图是我们根据词频得到的一棵二叉树的部分结构：\n",
    "  \n",
    "    <img src=\"./source/Chap7/Huffman二叉树.svg\" width=600>\n",
    "  \n",
    "    此外，对于每个非叶子节点，它们还需要存储一个**隐藏的可训练参数** $\\theta\\in\\mathbb{R}^p$，它与词元嵌入的表示 $v_i$ 具有相同的维度\n",
    "  * 用 $L(w)$ 表示二叉树中词元 $w$ 所在的叶节点深度，例如 $L(w_3) = 4$，设 $n(w,j)$ 表示到达词元 $w$ 的路径上的第 $j$ 个节点，其保存的参数为 $\\theta_{n(w,j)}$，则**条件概率被建模为**：\n",
    "    $$\n",
    "    P(w_o | w_c) = \\prod_{j=1}^{L(w_o) - 1} \\sigma\\left( \\text{isLeft}(n(w_o,j),n(w_o,j+1))\\cdot \\theta_{n(w_o,j)}^T v_c \\right)\n",
    "    $$\n",
    "    其中，$\\sigma$ 是 `sigmoid` 函数，$\\text{isLeft}$ 函数判断路径中下一个节点 $n(w_o,j+1)$ 是否是 $n(w_o,j)$ 的左子节点，**是取 1，否取 -1**\n",
    "  * 对于给定中心词 $w_c$，向上图中生成 $w_3$ 的条件概率可以写作：\n",
    "    $$\n",
    "    P(w_3 | w_c) = \\sigma(u_{n(w_3,1)}^Tv_c)\\cdot \\sigma(-u_{n(w_3,2)}^Tv_c)\\cdot \\sigma(u_{n(w_3,3)}^Tv_c)\n",
    "    $$\n",
    "  * 由于 $\\sigma(x) + \\sigma(-x) = 1$，基于中心词 $w_c$ 生成词表中所有可能的词的条件概率和等于 1，这相当于遍历了整棵二叉树：\n",
    "    $$\n",
    "    \\sum_{i \\in\\mathcal{V}} P(w_i | w_c) = 1\n",
    "    $$\n",
    "  * 通过这种二叉树的技巧，**树的深度大约为** $\\mathcal{O}(\\log_2 |\\mathcal{V}|)$，因此，当词表大小很大时，**使用分层 softmax 这种近似算法也能使得计算复杂度大大降低**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2) 预训练数据集**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2.1) 导入数据，创建词表**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们演示使用 `PTB`（`Penn Tree Bank`，华尔街日报文章数据集）数据集来**预训练一个 word2vec 模型**\n",
    "* 在深度学习中，**预训练**（**Pre-Training**）希望让模型在一些较为通用的数据集上训练以**获得通用的特征抽取能力**\n",
    "  * 例如在 CNN 中，在 `ImageNet` 预训练过的 ResNet，VGGNet 等模型就有很好的泛化能力，我们只需要**借助它们，根据下游任务进行微调**\n",
    "  * word2vec 可以作为文本处理方面的预训练模型，它可以用于抽取词元的特征\n",
    "* `PTB` 语料库分为训练集、验证集和测试集，文本文件的每一行表示由空格分隔的一句话\n",
    "* 在该任务中，我们**以每个单词作为一个词元**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ptb(path : str):\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.read()\n",
    "    # 按照换行符分割，抽取得到每个句子\n",
    "    # 然后按照空格分割，得到每个单词，完成分词\n",
    "    # 由于 PTB 数据集非常干净，所以不需要做其他的预处理\n",
    "    return [sentence.split() for sentence in lines.split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "句子数目： 42069\n"
     ]
    }
   ],
   "source": [
    "sentences = load_ptb(path=\"./source/data/text/ptb/ptb.train.txt\")\n",
    "print(\"句子数目：\", len(sentences))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与所有自然语言处理任务相同，我们需要创建词表 `Vocab`，设置 `min_freq = 10` 过滤掉出现次数较少的词元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词表大小： 6722\n"
     ]
    }
   ],
   "source": [
    "vocab = utils.Vocab(sentences, min_freq=10)\n",
    "print(\"词表大小：\", len(vocab))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2.2) 下采样**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一些上下文连接词，例如 `the`, `a`, `in` 等，**在文本中出现的频率非常高**（在大型文本数据集中，出现次数可能超过几十亿次）\n",
    "* 但这些词在上下文窗口中通常与其他有意义的词元一同出现，**提供的信息又很少**，无含义的高频词可能会**让模型对概率的建模偏向于选择预测这些无意义的词**\n",
    "* 此外，这些无含义的高频词，会**占用很多的计算时间，使得模型的训练速度变慢**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一种解决思路是，对文本中出现的高频词进行**下采样**，舍数据集中每个词 $w_i$ 的次数与总词数的比例为 $f(w_i)$，则**下采样按照概率** $P(w_i)$ **丢弃词元**：\n",
    "$$\n",
    "P(w_i) = \\max\\left( 1 - \\sqrt{\\frac{t}{f(w_i)}},0 \\right)\n",
    "$$\n",
    "常数 $t$ 是超参数，例如取 $t = 10^{-4}$，只有当 $f(w_i) > t$ 时，高频词 $w_i$ 才能被丢弃，并且**出现频率** $f(w_i)$ **越高，被丢弃的概率越大**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数 `down_sampling()` 用于实现对高频词的下采样，函数已经写入 `/source/code/ch7.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_sampling(sentences : list, vocab, t : float = 1e-4):\n",
    "    from collections import Counter\n",
    "    import random\n",
    "    # 下采样高频词\n",
    "\n",
    "    # 排除掉未知词元 <unk>\n",
    "    sentences = [[token for token in line if vocab[token] != vocab['<unk>']] for line in sentences]\n",
    "    \n",
    "    # 统计词频\n",
    "    \n",
    "    counter = Counter([token for line in sentences for token in line])\n",
    "    num_tokens = sum(counter.values()) # 总词数\n",
    "\n",
    "    # 定义一个辅助函数，如果在下采样过程中保留该词，则返回 True\n",
    "    def keep(token):\n",
    "        return random.uniform(0, 1) < np.sqrt(t / counter[token] * num_tokens)\n",
    "    \n",
    "    # 对每个句子的词元进行下采样\n",
    "    return [[token for token in line if keep(token)] for line in sentences], counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对高频词进行下采样\n",
    "down_sampled_sentences, counter = down_sampling(sentences, vocab)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "绘制下采样前后，句子长度的分布可以看到，**下采样通过删除高频词能够显著缩短句子，从而加速模型的训练**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f6814ed9f70>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(5, 3))\n",
    "# 统计句子长度\n",
    "origin = [len(line) for line in sentences]\n",
    "down_sampled = [len(line) for line in down_sampled_sentences]\n",
    "# 绘制直方图\n",
    "_,_,patches = plt.hist([origin, down_sampled], bins=10, label=[\"origin\", \"down sampled\"])\n",
    "# 显示条纹\n",
    "for patch in patches[1]:\n",
    "    patch.set_hatch('///')\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数 `compare_token_counts` 帮助我们对比下采样前后，词元出现频率的变化情况\n",
    "* 高频词元被大幅下采样，而低频词元不受影响，继续保留"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_token_counts(token):\n",
    "    print(\"词元 {} 的数量：\\n下采样前：{}\\n下采样后：{}\".format(token, \n",
    "        sum([line.count(token) for line in sentences]), \n",
    "        sum([line.count(token) for line in down_sampled_sentences])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词元 the 的数量：\n",
      "下采样前：50770\n",
      "下采样后：2055\n",
      "词元 join 的数量：\n",
      "下采样前：45\n",
      "下采样后：45\n"
     ]
    }
   ],
   "source": [
    "compare_token_counts(\"the\")\n",
    "compare_token_counts(\"join\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们可以将词元映射到词表索引，处理为语料 `corpus`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "语料大小： 42069\n",
      "第10个句子： [66, 278, 1902, 1442, 2370, 3145, 716, 108, 5582, 1295]\n"
     ]
    }
   ],
   "source": [
    "corpus = [vocab[line] for line in down_sampled_sentences]\n",
    "print(\"语料大小：\", len(corpus))\n",
    "print(\"第10个句子：\", corpus[9])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(3) 抽取中心词和上下文**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数 `create_centers_and_context` 用于构造中心词和上下文：\n",
    "* 我们依次处理每条语句，并把每条语句的每个词元都作为中心词\n",
    "* 生成中心词对应的上下文时，我们随机从范围 `[1, max_window_size]` 选取窗口长度，然后**从中心词两侧扩散**，得到上下文窗口\n",
    "  * 需要**确保窗口范围不会小于 0，也不会大于句子长度**\n",
    "  * 为了确保至少有一个上下文词，句子的长度必须大于 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_centers_and_context(corpus : list, max_window_size : int=2):\n",
    "    # 抽取中心词和上下文词\n",
    "    import random\n",
    "\n",
    "    # 初始化中心词和上下文词\n",
    "    centers, contexts = [], []\n",
    "    # 遍历每个句子\n",
    "    for line in corpus:\n",
    "        # 如果句子长度小于 2，跳过\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        # 遍历句子中的每个词元，将其作为中心词\n",
    "        centers += line # 先将句子中的词元添加到中心词列表中\n",
    "        for index, center in enumerate(line):\n",
    "            # 随机选择窗口大小\n",
    "            window_size = random.randint(1, max_window_size)\n",
    "            # 确认上下文词的索引\n",
    "            indices = list(range(max(0, index - window_size), min(len(line), index + 1 + window_size)))\n",
    "            indices.remove(index) # 移除中心词\n",
    "            contexts.append([line[idx] for idx in indices]) # 添加上下文词\n",
    "    return centers, contexts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以创建一个测试语料库，观察中心词和上下文词的抽取逻辑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "语料： [[0, 1, 2, 3, 4], [7, 8, 9]]\n",
      "中心词 0 的上下文词是： [1, 2]\n",
      "中心词 1 的上下文词是： [0, 2, 3]\n",
      "中心词 2 的上下文词是： [0, 1, 3, 4]\n",
      "中心词 3 的上下文词是： [2, 4]\n",
      "中心词 4 的上下文词是： [3]\n",
      "中心词 7 的上下文词是： [8]\n",
      "中心词 8 的上下文词是： [7, 9]\n",
      "中心词 9 的上下文词是： [7, 8]\n"
     ]
    }
   ],
   "source": [
    "fake_corpus = [list(range(5)), list(range(7, 10))]\n",
    "print(\"语料：\", fake_corpus)\n",
    "centers, contexts = create_centers_and_context(fake_corpus, max_window_size=2)\n",
    "for center, context in zip(centers, contexts):\n",
    "    print(\"中心词\", center, \"的上下文词是：\", context)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于 `PTB` 数据集，我们可以设置最大窗口长度为 `max_window_size = 5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中心词-上下文词对数目： 342754\n"
     ]
    }
   ],
   "source": [
    "centers, contexts = create_centers_and_context(corpus, max_window_size=5)\n",
    "print(\"中心词-上下文词对数目：\", len(centers))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(4) 负采样**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来实现在**近似训练**中提到的**负采样技巧**\n",
    "* 创建 `RandomGenerator` 类根据采样权重 `sampling_weights`在顺序索引中采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomGenerator:\n",
    "    def __init__(self, sampling_weights, buffer_size : int=10000) -> None:\n",
    "        self.population = list(range(len(sampling_weights))) # 总体的顺序索引\n",
    "        self.samping_weights = sampling_weights\n",
    "        self.candidates = []\n",
    "        self.i = 0\n",
    "        self.buffer_size = buffer_size # 缓存大小\n",
    "    \n",
    "    # 采样\n",
    "    def draw(self):\n",
    "        import random\n",
    "        # 通过递归，直到采样数目达到 \n",
    "        if self.i == len(self.candidates):\n",
    "            # 缓存采样结果\n",
    "            self.candidates = random.choices(self.population, weights=self.samping_weights, k=self.buffer_size)\n",
    "            self.i = 0 # 重置索引\n",
    "        # 每次返回一个采样结果，并将索引加 1\n",
    "        self.i += 1\n",
    "        return self.candidates[self.i - 1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例如我们有三个索引 `[0,1,2]`，如下方式可以得到一个采样比例为 `2 : 3 : 4` 的抽样结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 1]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = ch7.RandomGenerator([1, 3, 5])\n",
    "[generator.draw() for _ in range(20)] # 采样 20 次"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`negative_sampling()` 用于实现负采样，负采样的个数 `K` 作为超参数可以控制\n",
    "* 采样分布 $P(w)$ 根据 word2vec 原论文中的建议，可以**设置为词元出现频率的 0.75 次幂**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sampling(contexts, vocab, counter, K : int=5):\n",
    "    \"\"\"\n",
    "    ## negative_sampling\n",
    "        上下文词元负采样\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    contexts : list\n",
    "        上下文词元列表\n",
    "    vocab : Vocab\n",
    "        词表\n",
    "    counter : dict\n",
    "        词频统计\n",
    "    K : int, default = 5\n",
    "        负采样数目\n",
    "    \"\"\"\n",
    "    \n",
    "    # 计算采样权重，确定采样分布 P(w)\n",
    "    # 四个特殊词元不会出现在词频统计中，因此使用 get 方法获取词频时，指定默认值 0\n",
    "    # 0**0.75 = 0，因此这四个特殊词元的采样权重为 0\n",
    "    sampling_weights = [counter.get(vocab.to_tokens(i),0)**0.75 for i in range(len(vocab))]\n",
    "    # 初始化采样器和负样本\n",
    "    sampler = ch7.RandomGenerator(sampling_weights)\n",
    "    negatives = []\n",
    "    # 遍历每个上下文词元\n",
    "    for context in contexts:\n",
    "        negative = [] # 初始化当前上下文词元的负样本\n",
    "        while len(negative) < len(context) * K: # 每个上下文词元需要采样 K 个负样本\n",
    "            # 采样\n",
    "            neg = sampler.draw()\n",
    "            # 如果采样到的词元是上下文词元，跳过\n",
    "            if neg not in context:\n",
    "                negative.append(neg)\n",
    "        negatives.append(negative)\n",
    "\n",
    "    return negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_samples = negative_sampling(contexts, vocab, counter, K=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5) 组件整合，拼装为批量数据**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`create_batch_data()` 用于创建和组装批量数据，我们需要将中心词，上下文词（正样本）和随机采样的噪声词（负样本）填充到相同的维度拼在一起\n",
    "* 输入 `data` 是一个列表，长度等于样本量，**每个元素包含三个部分**：中心词，上下文词和噪声词\n",
    "* 在批量中，第 $i$ 个样本的中心词，可能有 $n_i$ 个上下文词和 $m_i$ 个噪声词，由于**上下文窗口不固定**，$n_i + m_i$ 也各不相同\n",
    "  * 我们用 `context_negatives` 将上下文和噪声词连接起来，并**通过填充 0 使其扩充到固定长度** `max_len = max_i(n_i + m_i)`\n",
    "  * 在计算损失时，我们类似之前 RNN 和 Transformer，**创建掩码** `masks`，将填充的元素掩蔽，从而**在损失的计算中过滤掉它们**\n",
    "  * 同理，为了区分正样本和负样本，我们也创建一个掩码 `labels`，当 `labels = 1` 时，其对应上下文词，即正样本\n",
    "  * `masks` 和 `labels` 与 `context_negatives` 有相同的形状，它们的元素一一对应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_data(data : list):\n",
    "    # 计算 max_len\n",
    "    max_len = max([len(context) + len(negative) for center, context, negative in data])\n",
    "    # 初始化中心词、上下文词和负样本的索引列表，以及掩码\n",
    "    centers, contexts_negatives, masks, labels = [], [], [], []\n",
    "    for center, context, negative in data:\n",
    "        valid_len = len(context) + len(negative) # 当前样本的有效长度\n",
    "        centers += [center] # 添加中心词\n",
    "        contexts_negatives += [context + negative + [0] * (max_len - valid_len)] # 添加上下文词和负样本\n",
    "        masks += [[1] * valid_len + [0] * (max_len - valid_len)] # 有效填充的掩码\n",
    "        labels += [[1] * len(context) + [0] * (max_len - len(context))] # 正负样本掩码\n",
    "    return tf.reshape(tf.constant(centers), (-1,1)), tf.constant(contexts_negatives), tf.constant(masks), tf.constant(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centers = tf.Tensor(\n",
      "[[1]\n",
      " [1]], shape=(2, 1), dtype=int32)\n",
      "contexts_negatives = tf.Tensor(\n",
      "[[2 2 3 3 3 3]\n",
      " [2 2 2 3 3 0]], shape=(2, 6), dtype=int32)\n",
      "masks = tf.Tensor(\n",
      "[[1 1 1 1 1 1]\n",
      " [1 1 1 1 1 0]], shape=(2, 6), dtype=int32)\n",
      "labels = tf.Tensor(\n",
      "[[1 1 0 0 0 0]\n",
      " [1 1 1 0 0 0]], shape=(2, 6), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x1 = (1, [2, 2], [3, 3, 3, 3])\n",
    "x2 = (1, [2, 2, 2], [3, 3])\n",
    "batch = ch7.create_batch_data((x1, x2))\n",
    "\n",
    "names = ['centers', 'contexts_negatives', 'masks', 'labels']\n",
    "for name, data in zip(names, batch):\n",
    "    print(name, '=', data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们将以上所有组件包装成为 `create_PTBdataloader()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_PTBdataloader(path : str, batch_size : int=32, max_window_size : int=5, num_noise_words : int=5):\n",
    "    # 加载文件，并进行分词\n",
    "    sentences = load_ptb(path)\n",
    "    vocab = utils.Vocab(sentences, min_freq=10) # 创建词表\n",
    "    # 下采样高频词元\n",
    "    down_sampled, counter = ch7.down_sampling(sentences,vocab)\n",
    "    corpus = [vocab[line] for line in down_sampled] # 将词元转换为索引\n",
    "    centers, contexts = ch7.create_centers_and_context(corpus, max_window_size) # 创建中心词和上下文词\n",
    "    negatives = ch7.negative_sampling(contexts, vocab, counter, num_noise_words) # 创建负样本\n",
    "\n",
    "    # 数据集 PTB 的 DataLoader\n",
    "    class PTBDataset:\n",
    "        def __init__(self, centers, contexts, negatives) -> None:\n",
    "            assert len(centers) == len(contexts) == len(negatives) # 确保三个列表长度相同\n",
    "            self.centers = centers\n",
    "            self.contexts = contexts\n",
    "            self.negatives = negatives\n",
    "            self.data = list(zip(self.centers, self.contexts, self.negatives)) # 将三个列表打包为列表\n",
    "        \n",
    "        # 每次返回一份批量数据\n",
    "        def __iter__(self):\n",
    "            # 打乱数据\n",
    "            import random\n",
    "            random.shuffle(self.data)\n",
    "            # 根据 batch_size 划分数据\n",
    "            for i in range(0, len(self.data), batch_size):\n",
    "                yield create_batch_data(self.data[i:i+batch_size])\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.centers)\n",
    "\n",
    "        def create_dataset(self):\n",
    "            dataset = tf.data.Dataset.from_generator(self.__iter__, output_types=(tf.int32,tf.int32,tf.int32,tf.int32))\n",
    "            return dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    dataset = PTBDataset(centers, contexts, negatives)\n",
    "    return dataset.create_dataset(), vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter, vocab = ch7.create_PTBdataloader(\n",
    "    path='./source/data/text/ptb/ptb.train.txt', batch_size=32, max_window_size=5, num_noise_words=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以观察到，每个批量数据的样本量都为 `batch_size`，但是**不同批量的** `contexts_negatives` **长度可能会不一样**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centers shape:  (32, 1)\n",
      "contexts_negatives shape:  (32, 54)\n",
      "masks shape:  (32, 54)\n",
      "labels shape:  (32, 54)\n",
      "\n",
      "centers shape:  (32, 1)\n",
      "contexts_negatives shape:  (32, 60)\n",
      "masks shape:  (32, 60)\n",
      "labels shape:  (32, 60)\n",
      "\n",
      "centers shape:  (32, 1)\n",
      "contexts_negatives shape:  (32, 60)\n",
      "masks shape:  (32, 60)\n",
      "labels shape:  (32, 60)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_cnts = 0\n",
    "for batch in data_iter:\n",
    "    batch_cnts += 1\n",
    "    for name, data in zip(names, batch):\n",
    "        print(name, 'shape: ', data.shape)\n",
    "    print()\n",
    "    if batch_cnts >= 3:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(3) 模型训练**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们定义 word2vec 模型及其计算逻辑，然后训练模型，我们仅以跳元模型为例进行说明"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我们需要一个 `Embedding` 层将词元整数索引映射到给定维度 `embed_size` 的词向量空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3, 4), dtype=float32, numpy=\n",
       "array([[[ 0.00821521,  0.00616704,  0.03815914,  0.01318972],\n",
       "        [-0.02145312,  0.04401647,  0.04825136,  0.04198031],\n",
       "        [ 0.02694089, -0.0177655 ,  0.04446788, -0.02161142]],\n",
       "\n",
       "       [[-0.02145312,  0.04401647,  0.04825136,  0.04198031],\n",
       "        [ 0.02694089, -0.0177655 ,  0.04446788, -0.02161142],\n",
       "        [ 0.03820852, -0.01937146,  0.04628712,  0.03335824]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input_dim 为词表大小 vocab_size，output_dim 为词向量维度\n",
    "embed = tf.keras.layers.Embedding(input_dim=20, output_dim=4)\n",
    "x = tf.constant([[0,1,2],[1,2,3]])\n",
    "embed(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，定义**跳元模型的前向推理过程**：\n",
    "* `skip_gram()` 的输入参数包含中心词 `centers`，上下文词与噪声词的拼接 `contexts_negatives`\n",
    "* 我们还向函数传递了负责词元**中心词表示和上下文表示的两个嵌入层** `embed_v, embed_u`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_gram(centers, contexts_negatives, embed_v, embed_u):\n",
    "    # centers 形状：(batch_size, 1)\n",
    "    # contexts_negatives 形状：(batch_size, max_len)\n",
    "    \n",
    "    v = embed_v(centers) # 中心词的词向量表示，形状：(batch_size, 1, embed_size)\n",
    "    u = embed_u(contexts_negatives) # 上下文词和负样本的词向量表示，形状：(batch_size, max_len, embed_size)\n",
    "    # 做批量矩阵乘法，得到中心词和上下文词的内积\n",
    "    # 形状：(batch_size, 1, max_len)\n",
    "    pred = tf.matmul(v, tf.transpose(u, perm=[0,2,1]))\n",
    "    return pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以将它们简单的封装为模型 `word2vecSkipGram`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class word2vecSkipGram(tf.keras.Model):\n",
    "    def __init__(self, vocab_size : int, embed_size : int, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.embed_size = embed_size\n",
    "        self.embed_v = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_size, name='embed_v')\n",
    "        self.embed_u = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_size, name='embed_u')\n",
    "    \n",
    "    def call(self, centers : tf.Tensor, contexts_negatives : tf.Tensor, *args, **kwargs):\n",
    "        return skip_gram(centers, contexts_negatives, embed_v=self.embed_v, embed_u=self.embed_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出形状： (32, 1, 54)\n"
     ]
    }
   ],
   "source": [
    "word2vec = word2vecSkipGram(vocab_size=len(vocab), embed_size=64)\n",
    "for batch in data_iter:\n",
    "    print(\"输出形状：\",word2vec(batch[0], batch[1], training=False).shape)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们为上下文和负采样得到的噪声词定义了掩码 `masks` 和 `labels`，因此**需要重新调整一下损失函数的计算**\n",
    "* 我们的模型 `word2vec` 的输出 `preds` 没有进行概率变换，是原始的 logits 向量，**每个元素预测对应的上下文词是正样本还是负样本**（**二分类问题**）\n",
    "* 假设 `preds` 的形状是 `(batch_size, max_len)`，则上述损失计算的形状也为 `(batch_size, max_len)`，**随后用相同形状的** `masks` **去除掉那些我们为了补全序列长度到** `max_len` **的无效词元的损失**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidCELoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def __call__(self, labels, preds, masks, *args, **kwargs):\n",
    "        # labels, preds, masks 形状：(batch_size, max_len)\n",
    "        masked_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=preds) * masks # 形状：(batch_size, max_len)\n",
    "        loss = tf.reduce_sum(masked_loss, axis=1) # 形状：(batch_size,)\n",
    "        return loss / tf.reduce_sum(masks, axis=1) # 形状：(batch_size,)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们定义模型的训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(model, data_iter, lr : float=0.01, Epochs : int=10, verbose : int=1):\n",
    "    # 定义优化器和损失函数\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss_func = SigmoidCELoss()\n",
    "\n",
    "    # 展示训练进度\n",
    "    animator = utils.Animator(xlabel='epoch', ylabel='loss', xlim=[1, Epochs])\n",
    "\n",
    "    for epoch in range(Epochs):\n",
    "        # 存储每个迭代周期的损失和样本量\n",
    "        loss_batch = tf.constant(0.0)\n",
    "        train_samples = tf.constant(0)\n",
    "\n",
    "        for i, batch in enumerate(data_iter):\n",
    "            centers, contexts_negatives, masks, labels = batch\n",
    "            with tf.GradientTape() as tape:\n",
    "                # preds 形状：(batch_size, 1, max_len)\n",
    "                preds = model(centers, contexts_negatives, training=True)\n",
    "                # 变换 preds 形状，去掉中间的维度，得到形状：(batch_size, max_len)\n",
    "                preds = tf.squeeze(preds, axis=1)\n",
    "                # 为了满足损失计算的要求，需要将 labels, masks 变为 tf.float32 类型\n",
    "                loss = loss_func(tf.cast(labels, tf.float32), preds, tf.cast(masks, tf.float32))\n",
    "            weights = model.trainable_variables\n",
    "            grads = tape.gradient(loss, weights)\n",
    "            optimizer.apply_gradients(zip(grads, weights))\n",
    "\n",
    "            # 计算损失\n",
    "            loss_batch += tf.reduce_sum(loss)\n",
    "            train_samples += tf.reduce_sum(masks)\n",
    "\n",
    "        if epoch == 0 or (epoch + 1) % verbose == 0:\n",
    "            train_loss = loss_batch.numpy() / train_samples.numpy()\n",
    "            animator.add(epoch + 1, (train_loss,))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入数据集，然后训练 `word2vec` 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter, vocab = ch7.create_PTBdataloader(\n",
    "    path='./source/data/text/ptb/ptb.train.txt', batch_size=512, max_window_size=5, num_noise_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = ch7.word2vecSkipGram(vocab_size=len(vocab), embed_size=64)\n",
    "model = ch7.train_word2vec(model, data_iter, Epochs=10, verbose=1, lr=0.001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们可以通过词嵌入的向量，利用余弦相似度来找到与目标词 `query_token` 最相似的 top K 个词，函数 `get_similar_tokens` 实现该功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_tokens(query_token : str, embed : tf.keras.layers.Embedding, vocab, top_k : int=3):\n",
    "    \"\"\"\n",
    "    ## get_similar_tokens\n",
    "        从词表中找出与查询词语义最相近的词\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    query_token : str\n",
    "        查询目标词\n",
    "    embed : tf.keras.layers.Embedding\n",
    "        word2vec 模型的嵌入层\n",
    "    vocab : Vocab\n",
    "        词表\n",
    "    top_k : int, default = 5\n",
    "        返回最相近的 top_k 个词\n",
    "    \"\"\"\n",
    "    # 获取 query_token 的词向量\n",
    "    weights = embed.get_weights()[0] # 形状：(vocab_size, embed_size)\n",
    "    query = weights[vocab[query_token]] # 形状：(embed_size,)\n",
    "    # 计算余弦相似度，1e-9 做数值保护\n",
    "    # weights @ query 形状：(vocab_size, 1)\n",
    "    cos = (weights @ query[:, None])[:,0] / tf.sqrt(tf.reduce_sum(weights * weights, axis=1) * tf.reduce_sum(query * query) + 1e-9)\n",
    "    \n",
    "    # 通过 argsort 函数返回 top_k 个最大的元素的索引\n",
    "    topk_tokens = tf.argsort(cos, axis=0, direction='DESCENDING')[:top_k+1].numpy().tolist()\n",
    "    for i in topk_tokens:\n",
    "        print('cosine sim = %.3f: %s' % (cos[i], vocab.idx_to_token[i]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在使用嵌入层时，`SkipGram` 使用中心词表示，因此我们传递 `model.embed_v` 给查询函数 `get_similar_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim = 1.000: chip\n",
      "cosine sim = 0.758: intel\n",
      "cosine sim = 0.696: microprocessor\n",
      "cosine sim = 0.645: fortunes\n"
     ]
    }
   ],
   "source": [
    "ch7.get_similar_tokens('chip', model.embed_v, vocab, top_k=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **练习**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
