{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chap 7：自然语言处理**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-13 15:10:24.558634: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-13 15:10:24.888328: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-13 15:10:24.940629: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-06-13 15:10:26.698216: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/chenguangze/software/miniconda3/lib/:/home/chenguangze/software/miniconda3/lib/:/home/chenguangze/software/miniconda3/envs/tensorflow/lib/\n",
      "2023-06-13 15:10:26.698449: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/chenguangze/software/miniconda3/lib/:/home/chenguangze/software/miniconda3/lib/:/home/chenguangze/software/miniconda3/envs/tensorflow/lib/\n",
      "2023-06-13 15:10:26.698470: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import collections\n",
    "from source.code import ch7\n",
    "from source.code import utils \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果你要在服务器上和别人共用GPU，可以设置你需要的显存资源\n",
    "utils.gpu_limitation_config(memory=30,device=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7.3 BERT**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7.3.1 BERT模型**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1) 从上下文无关到上下文敏感**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在前面介绍的所有的词嵌入预训练模型中，它们都是**上下文无关的词嵌入模型**，给定任何词元 $x$，任何词元的嵌入表示由函数 $f(x)$ 得到，**嵌入函数仅以词元** $x$ **为输入**，例如：\n",
    "* `we all know that apple is healthy`\n",
    "* `we all know that apple is a technology company`\n",
    "\n",
    "同一个单词 `apple` 在不同的上下文语义中表达不同的含义，它们**应当被嵌入到不同的词向量**，而由于之前的词嵌入模型的映射 $f$ 仅以 $x$ 为输入，所以单词 `apple` 会被映射到相同的词向量"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，另一种词嵌入模型被称为**上下文敏感**，**词的表示嵌入会考虑词元的上下文变量** $c(x)$，将映射改为 $f(x,c(x))$，流行的上下文敏感表示有：\n",
    "* **TagLM**（language-model-augmented sequence tagger，语言模型增强的序列标记器）\n",
    "* **CoVe**（Context Vectors，上下文向量）\n",
    "* **ELMo**（Embeddings from Language Models，语言模型嵌入）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以 ELMo 为例，人们通常将整个文本序列作为输入，**ELMo 使用来自预训练的双向 LSTM**，将序列中的每个词元映射到对应的词向量，然后该向量表示**作为附加的特征添加到下游任务的现有监督模型中**，用于不同的 NLP 任务\n",
    "* 例如，**将来自 ELMo 的词嵌入向量与现有模型的词元原始表示（例如 GloVe）拼接起来**\n",
    "* 加入 ELMo 模型时，通常会**冻结 ELMo 的双向 LSTM 中的所有参数**\n",
    "* **现有的下游监督模型专门为特定的 NLP 任务定制和构造**，例如情感分析、自然语言推断，命名实体识别等\n",
    "\n",
    "EMLo 这种上下文词向量的加入，在当时改进了很多模型在各 NLP 任务上的表现"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2) 从特定子任务到通用不可知任务**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EMLo 的加入虽然改善了 NLP 模型的性能，但**每个 NLP 任务都需要依赖于特定设计的模型架构**，为每个 NLP 任务设计一个特定的架构并不是一件容易的事情，并且，**从通用文本语义理解的角度思考，人们也希望获得统一和通用的语言模型**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GPT**（**Generative Pre-training Transformer**，生成式预训练 Transformer）为上下文的敏感表示设计了通用的**任务无关模型**\n",
    "* GPT 以 Transformer 的解码器为基础，通过在大量文本上做预训练，以获得词元的上下文敏感词嵌入模型\n",
    "* 预训练任务为**语言模型**，即**给定上下让模型预测下文**（自监督任务），GPT认为这样的预训练任务能模型获得通用的词元表示能力\n",
    "* GPT 的**输出向量将被应用到附加的全连接层**，然后与特定的 NLP 预测任务标签计算损失\n",
    "* 与 ELMo 不同，GPT 在下游有监督学习过程中，不会冻结 Transformer 解码器的参数，Transformer 解码器中的所有参数会**进行微调**，而用于下游任务的全连接网络会从头开始学习\n",
    "\n",
    "GPT 系列模型的具体细节，我们将在后面的**大语言模型**（**Large Languange Model**，**LLM**）专题中介绍"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然而，由于**语言模型 LM**的**自回归特征**，GPT 使用的 **Transformer 解码器每次只能从左到右完成对词元的编码**，即它没法完成**双向编码**，仍然以前面的两个句子为例：\n",
    "* `we all know that apple is healthy`\n",
    "* `we all know that apple is a technology company`\n",
    "\n",
    "对 GPT 的 Transformer 解码器而言，单词 `apple` 被编码时，**都只能看到相同的上文**，因此两个句子中的 `apple` 会得到相同的词嵌入表示，但它们在句子中的含义不同"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(3) BERT**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT（**来自 Transformers 的双向编码器表示**）结合了 ELMo 的上下文双向编码优点，和 GPT 的任务无关驱动的优点，它对上下文进行双向编码，并且对于大多数 NLP 任务，只需要最少的架构改变\n",
    "* 在下游任务的监督学习过程中，BERT 与 GPT 在两个方面相似\n",
    "  * BERT 将词的表示输入到一个下游输出网络中（例如全连接层），**根据任务性质对模型架构做小改的改动（例如预测是针对每个词，还是针对整个句子）**\n",
    "  * BERT 与 GPT 一样，需要**对 Transformer 编码器的所有参数做微调**，而额外的输出层将从头开始训练"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下图展示了 ELMo，GPT 和 BERT 的对比"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./source/Chap7/BERT对比.svg\" width=700>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们具体来介绍 BERT 模型"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，BERT 的输入文本有**两种格式**：\n",
    "* **单个文本输入**：此时 BERT 接收一个文本序列 `tokens_a`，并且在其头部和尾部添加上特征词元 `<cls>` 和 `<sep>`，即 `<cls> tokens_a <sep>`\n",
    "* **文本对输入**：此时 BERT 接收两个文本序列 `tokens_a`，`tokens_b`，并且在头部，中间和尾部添加上特殊词元 `<cls>`，`<sep>`，得到 `<cls> tokens_a <sep> tokens_b <sep>`\n",
    "\n",
    "我们把以上两种称为**BERT 输入序列**，从而与普通的文本序列区分开来，特殊词元 `<cls>` 被称为**任务词元**，而 `<sep>` 是**分割词元**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了区分文本对，BERT 根据序列学习得到的**片段编码嵌入** $\\mathbf{e}_a,\\mathbf{e}_b$ 分别添加到 BERT 文本对的 `tokens_a` 和 `tokens_b` 上，对于单文本输入的情况，则只使用 $\\mathbf{e}_a$，函数 `get_BERT_tokens_and_segments` 接收一个句子或两个句子，加工得到 BERT 所需的输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_BERT_tokens_and_segments(tokens_a, tokens_b):\n",
    "    tokens = [\"<cls>\"] + tokens_a + [\"<sep>\"]\n",
    "    # segments 用于区分两个句子，0 表示第一个句子，1 表示第二个句子\n",
    "    segments = [0] * (len(tokens_a) + 2)\n",
    "    if tokens_b is not None:\n",
    "        tokens += tokens_b + [\"<sep>\"]\n",
    "        segments += [1] * (len(tokens_b) + 1)\n",
    "    return tokens, segments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回顾在 Transformer 中，我们对词元的嵌入表示由两部分组成：`Embedding` 嵌入后的向量表示，**加上位置编码**，BERT 稍有不同，它**包含三部分组成**：\n",
    "* `Embedding` 层的**词嵌入结果**\n",
    "* 词元在序列中的**位置编码**，与 Transformer 不同的是，**BERT 使用了可学习的位置编码**，实践表明 Transformer 中的**正余弦位置编码**和可学习的位置编码取得的结果没有显著区别\n",
    "* BERT 还包含**段落嵌入向量**，因为 BERT 序列可能会包含文本对，为了**让模型能够区分出两段不同序列的前后关系**，需要在词元表示中加入段落嵌入向量"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./source/Chap7/bert的输入表示.svg\" width=800>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们类似于 `TransformerEncoder` 来实现一个 `BERTEncoder`，注意 `BERTEncoder` **使用段落嵌入和可学习的位置嵌入**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEncoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    ### BERTEncdoer\n",
    "    BERT 编码器\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size : int, num_hiddens : int, norm_shape : list, ffn_num_hiddens : int,\n",
    "                 num_heads : int, num_layers : int, dropout : float, use_bias : bool=True, max_len : int = 1000, \n",
    "                 trainable=True, name=None, dtype=None, dynamic=False, **kwargs):\n",
    "        super(BERTEncoder, self).__init__(trainable=trainable, name=name, dtype=dtype, dynamic=dynamic, **kwargs)\n",
    "        # 词元嵌入\n",
    "        self.token_embedding = tf.keras.layers.Embedding(vocab_size, num_hiddens)\n",
    "        # 段落嵌入\n",
    "        self.segment_embedding = tf.keras.layers.Embedding(2, num_hiddens)\n",
    "        # 生成可学习的位置编码\n",
    "        self.pos_embedding = tf.Variable(tf.random.normal(shape=(1, max_len, num_hiddens), dtype=tf.float32), trainable=True)\n",
    "        # 注意力层，沿用 Transformer 的实现\n",
    "        self.blocks = [\n",
    "            utils.EncoderBlock(num_hiddens, norm_shape, ffn_num_hiddens, num_heads, dropout, use_bias) for _ in range(num_layers)\n",
    "        ]\n",
    "    \n",
    "    def call(self, tokens, segments, valid_lens, **kwargs):\n",
    "        # tokens, segments 形状: (batch_size, seq_len)\n",
    "        # BERT 输入 = 词元嵌入 + 段落嵌入 + 位置嵌入\n",
    "        X = self.token_embedding(tokens) + self.segment_embedding(segments) + self.pos_embedding[:, :tokens.shape[1], :]\n",
    "        # 注意力层计算\n",
    "        for block in self.blocks:\n",
    "            X = block(X, valid_lens, **kwargs)\n",
    "        return X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来测试 `BERTEncoder` 的接口工作是否正常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 100\n",
    "encoder = BERTEncoder(\n",
    "    vocab_size=vocab_size, num_hiddens=256, norm_shape=[2], \n",
    "    ffn_num_hiddens=512, num_heads=8, num_layers=2, dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 8, 256])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 生成词元和段落\n",
    "tokens = tf.random.uniform((2,8), minval=0, maxval=vocab_size, dtype=tf.int32)\n",
    "segments = tf.constant([[0, 0, 0, 0, 1, 1, 1, 1], \n",
    "                        [0, 0, 0, 1, 1, 1, 1, 1]])\n",
    "encoded_X = encoder(tokens, segments, None)\n",
    "encoded_X.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(4) BERT 预训练任务**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT 包含两个预训练任务：**掩蔽语言模型**（**Masked Language Model**）和**下一句预测**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**任务一：掩蔽语言模型 MLM**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在前面介绍的各种模型中，我们大都是**用左侧的上文来预测接下来的词元**，BERT 为了**充分利用双向编码的上下文表示**，我们**随机掩蔽句子中的词元**，并**使用来自双向上下文的词元表示以自监督的方式预测被掩蔽的词元**，这个任务被称为**掩蔽语言模型**，具体来说，我们随机选择 15% 的词元进行掩蔽作为预测词元\n",
    "* 要预测一个掩蔽词元，而不造成标签泄漏而作弊，一个简单的方案是**用其他词元替换掉被掩蔽的词元**，例如**使用一个特殊词元** `<mask>`\n",
    "* 但是，特殊词元 `<mask>` 并**不会出现在真实的微调任务中**，认为引入高达 15% 比例的特殊词元 `<mask>` 会**导致训练集和测试集分布偏差，影响模型的真实表现**\n",
    "* 一种退而求其次的解决方案是，通过**加入噪声**来让 BERT 学习双向的上下文表示时**不那么偏向于选择掩蔽词元**，具体来说，例如在句子 `this moive is great` 中要掩蔽和预测 `great`，则：\n",
    "  * 80% 的时间将 `great` 替换为特殊词元 `<mask>`，即 `the movie is <mask>`\n",
    "  * 10% 的时间将 `great` **替换为随机单词**，即 `the movie is watch`\n",
    "  * 10% 的时间将 `great` **保持不变**，即保留 `the movie is great`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的类 `MaskedLM` 可以预测 BERT 预训练的 MLM 任务中的掩蔽标记，预测时**使用一个带有** `LayerNorm` **层的全连接网络**输出到 `vocab_size`\n",
    "* 在前向推理中，它**需要两个输入**：`BERTEncoder` 对句子中词元的编码表示，以及用于掩蔽和预测的词元位置，它将输出这些位置的预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLM(tf.keras.Model):\n",
    "    def __init__(self, vocab_size : int, num_hiddens : int, *args, **kwargs):\n",
    "        super(MaskedLM, self).__init__(*args, **kwargs)\n",
    "        self.mlp = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Dense(num_hiddens, activation='relu'), # 形状: (batch_size, seq_len, num_hiddens)\n",
    "            tf.keras.layers.LayerNormalization(), # 形状: (batch_size, seq_len, num_hiddens)\n",
    "            tf.keras.layers.Dense(vocab_size), # 形状: (batch_size, seq_len, vocab_size)\n",
    "            tf.keras.layers.Softmax() # 得到每个词元的概率分布\n",
    "        ])\n",
    "    \n",
    "    def call(self, X, pred_positions, training=None, mask=None):\n",
    "        # X 形状: (batch_size, seq_len, num_hiddens)\n",
    "        # pred_positions 形状: (batch_size, num_pred)\n",
    "        num_pred = pred_positions.shape[1] # 预测词元的个数\n",
    "        pred_positions = tf.reshape(pred_positions, shape=(-1,)) # 拉直为向量，形状: (batch_size * num_pred,)\n",
    "        batch_size = X.shape[0] # 批量大小\n",
    "\n",
    "        # 假设 batch_size = 2，num_pred = 3，则下面的 batch_idx = [0,0,0,1,1,1]\n",
    "        batch_idx = tf.range(0, batch_size) # 批量的索引 0,1,2,...，形状: (batch_size,)\n",
    "        batch_idx = tf.repeat(batch_idx, repeats=num_pred) # 每个批量重复 num_pred 次，形状: (batch_size * num_pred,)\n",
    "\n",
    "        # 选取需要预测的词元的隐藏状态，形状: (batch_size * num_pred, num_hiddens)\n",
    "        masked_X = tf.gather_nd(X, indices=tf.stack((batch_idx, pred_positions), axis=1))\n",
    "        masked_X = tf.reshape(masked_X, shape=(batch_size, num_pred, -1)) # 形状: (batch_size, num_pred, num_hiddens)\n",
    "        mlm_Y_hat = self.mlp(masked_X,training=training) # 形状: (batch_size, num_pred, vocab_size)\n",
    "        return mlm_Y_hat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以创建实例，测试 `MaskedLM` 的接口工作是否正常，`MaskedLM` 的**每个预测，结果的大小都等于** `vocab_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3, 100])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size, seq_len, num_hiddens = 100, 20, 256\n",
    "mlm = MaskedLM(vocab_size, num_hiddens)\n",
    "\n",
    "# BERT 编码后的输入\n",
    "encoded_X = tf.random.uniform((2, seq_len, num_hiddens)) # 形状: (batch_size, seq_len, num_hiddens)\n",
    "# 指定需要预测的词元的位置\n",
    "pred_positions = tf.constant([[1, 5, 2], \n",
    "                              [6, 1, 5]])\n",
    "mlm_Y_hat = mlm(encoded_X, pred_positions)\n",
    "mlm_Y_hat.shape # (batch_size, num_pred, vocab_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLM 也将使用**交叉熵损失函数来指导模型进行训练**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 假设 mlm_Y 是真实标签\n",
    "mlm_Y = tf.constant([[7, 8, 9],\n",
    "                     [2, 3, 4]])\n",
    "loss_func = tf.keras.losses.SparseCategoricalCrossentropy(reduction='none')\n",
    "loss_func(y_true=mlm_Y, y_pred=mlm_Y_hat).shape # (batch_size, num_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**任务二：下一句预测**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLM 有助于帮助 BERT 获得结合上下文的单词编码能力，但是**这个任务无法建模文本之间的逻辑关系**，为了帮助 BERT 理解两个文本序列之间的关系，**BERT 在预训练中考虑了下一句预测的二分类任务**，在为预训练生成句子对时，有一半的时间它们确实是连续的两句话（即**正类样本**），在另一半时间，第二个句子是从语料库随机抽取的（即**负类样本**）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NextSentencePred` 类使用一个全连接神经网络构造二元分类器，来**预测第二个句子是否是 BERT 输入序列的第一个句子的下一句话**\n",
    "* 由于 `Transformer` 编码器中使用**自注意力**，所以**特殊词元** `<cls>` **的 BERT 表示可以视为对输入的两个句子进行了编码**，因此，分类器将 `<cls>` 的 BERT 表示作为输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextSentencePred(tf.keras.Model):\n",
    "    def __init__(self, num_hiddens : int, *args, **kwargs):\n",
    "        super(NextSentencePred, self).__init__(*args, **kwargs)\n",
    "        self.classifier = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Dense(num_hiddens, activation='tanh'),\n",
    "            tf.keras.layers.Dense(2), # 二分类\n",
    "            tf.keras.layers.Softmax()\n",
    "        ])\n",
    "    \n",
    "    def call(self, cls):\n",
    "        # BERT 的 cls 表示，形状: (batch_size, num_hiddens)\n",
    "        return self.classifier(cls)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以创建实例，测试 `NextSentencePred` 的接口是否工作正常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 2])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, seq_len, num_hiddens = 4, 20, 256\n",
    "encoded_X = tf.random.uniform((batch_size, seq_len, num_hiddens))\n",
    "nsp = NextSentencePred(num_hiddens=num_hiddens) # 实例化 NextSentencePred 类\n",
    "\n",
    "# cls 放在文本序列的开头\n",
    "cls = encoded_X[:, 0, :] # 形状: (batch_size, num_hiddens)\n",
    "nsp_Y_hat = nsp(cls)\n",
    "nsp_Y_hat.shape # (batch_size, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下一句预测任务**依然使用交叉熵损失函数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 假设 mlm_Y 是真实标签\n",
    "nsp_Y = tf.constant([0, 1, 1, 0])\n",
    "loss_func = tf.keras.losses.SparseCategoricalCrossentropy(reduction='none')\n",
    "loss_func(y_true=nsp_Y, y_pred=nsp_Y_hat).shape # (batch_size, )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后我们再次强调，**BERT 的预训练任务所需要的标签都可以从预训练语料库中生成，无需人工标注**，因此这是一个**自监督任务**\n",
    "* 这也是目前大型神经网络模型的发展方向，即尽可能减少模型对人工标注数据集的依赖\n",
    "* 原始的 BERT 在图书语料库和英文维基百科上进行了预训练，这是两个非常庞大的数据集，分别包含 8 亿个单词和 25 亿个单词"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后我们将上面两个预训练任务代码整合，包装为 `BERTModel`，**最终的损失函数是 MLM 任务和下一句预测任务的损失函数的线性组合**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size : int, num_hiddens : int, norm_shape : list, ffn_num_hiddens : int,\n",
    "                 num_heads : int, num_layers : int, dropout : float, max_len : int=1000, *args, **kwargs):\n",
    "        super(BERTModel, self).__init__(*args, **kwargs)\n",
    "        # 创建 BERT 编码器\n",
    "        self.encoder = ch7.BERTEncoder(vocab_size, num_hiddens, norm_shape, ffn_num_hiddens, \n",
    "                                       num_heads, num_layers, dropout, max_len=max_len)\n",
    "        # 两个预训练任务\n",
    "        self.mlm = MaskedLM(vocab_size, num_hiddens)\n",
    "        self.nsp = NextSentencePred(num_hiddens)\n",
    "    \n",
    "    def call(self, tokens, segments, valid_lens=None, pred_positions=None, training=None):\n",
    "        encoded_X = self.encoder(tokens, segments, valid_lens, training=training)\n",
    "        if pred_positions is not None:\n",
    "            mlm_Y_hat = self.mlm(encoded_X, pred_positions, training=training)\n",
    "        else:\n",
    "            mlm_Y_hat = None\n",
    "        nsp_Y_hat = self.nsp(encoded_X[:, 0, :]) # cls = encoded_X[:, 0, :]\n",
    "        return encoded_X, mlm_Y_hat, nsp_Y_hat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7.3.2 BERT预训练数据集**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们演示通过一个较小的数据集 `WikiText-2` 来**构造 MLM 和下一句预测所需的数据集**\n",
    "* `WikiText-2` 每行代表一个段落，任意标点符号的前面都添加了空格符\n",
    "* 我们**保留至少有两句话的段落**\n",
    "* 我们简单起见，我们仅**用句号** `\" . \"` **作为分隔符来拆分句子**，还有一些更复杂的拆分策略，我们在习题中讨论"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_wikitext(path : str):\n",
    "    import random\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    # 用句号 . 作为分隔符，然后过滤掉长度小于 2 的序列\n",
    "    # 段落 paragraphs 的每个元素是包含大于两个句子的列表\n",
    "    paragraphs = [line.strip().lower().split(' . ') for line in lines if len(line.split(' . ')) >= 2]\n",
    "    random.shuffle(paragraphs)\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "段落数量: 15496\n",
      "第一段句子数量: 5\n"
     ]
    }
   ],
   "source": [
    "path = \"./source/data/text/wikitext-2/wiki.train.tokens\"\n",
    "paragraphs = read_wikitext(path)\n",
    "paragraphs = [utils.english_tokenize(paragraph) for paragraph in paragraphs] # 分词\n",
    "print(\"段落数量:\", len(paragraphs))\n",
    "print(\"第一段句子数量:\", len(paragraphs[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1) 下一句预测的数据**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数 `get_next_sentence` 接收 `sentence`，`next_sentence` 和 `paragraphs`\n",
    "* 50% 的几率，函数返回 `sentence` 和 `next_sentence`，保留句子的连续关系\n",
    "* 50% 的几率，将从 `paragraphs` 中随机挑选一个句子替换掉 `next_sentence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_sentence(sentence, next_sentence, paragraphs):\n",
    "    import random\n",
    "    if random.random() < 0.5:\n",
    "        is_next = True\n",
    "    else:\n",
    "        # paragraphs 是一个二维列表，每个元素是包含多个句子的段落\n",
    "        next_sentence = random.choice(random.choice(paragraphs))\n",
    "        is_next = False\n",
    "    return sentence, next_sentence, is_next"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过调用 `get_next_sentence` ，函数 `get_nsp_data_from_paragraph` 生成下一句预测任务的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nsp_data_from_paragraph(paragraph, paragraphs, max_len):\n",
    "    # paragraph 是一个段落的句子列表，每个句子是词元列表\n",
    "    # paragraphs 是包含多个段落的列表\n",
    "    nsp_data = []\n",
    "    for i in range(len(paragraph) - 1):\n",
    "        tokens_a, tokens_b, is_next = get_next_sentence(paragraph[i], paragraph[i + 1], paragraphs)\n",
    "        # 用 <cls> 和 <sep> 拼接句子\n",
    "        if len(tokens_a) + len(tokens_b) + 3 > max_len:\n",
    "            continue\n",
    "        tokens, segments = get_BERT_tokens_and_segments(tokens_a, tokens_b)\n",
    "        nsp_data.append((tokens, segments, is_next))\n",
    "    return nsp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "句子数量: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[False, False, True, True]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsp_data = get_nsp_data_from_paragraph(paragraphs[0], paragraphs, max_len=64)\n",
    "print(\"句子数量:\", len(nsp_data))\n",
    "[data[2] for data in nsp_data]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2) 掩蔽语言模型的数据**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数 `get_replace_mlm_tokens` 用于对词元列表 `tokens` 中的单词进行随机替换\n",
    "* `valid_pred_positions` 是**不包含特殊词元**的 BERT 输入序列的词元索引列表，BERT 中我们不能掩蔽特殊词元 `<cls>, <sep>`，掩蔽和预测它们没有意义\n",
    "* `num_mlm_preds` 是要掩蔽预测的词元数量（15% 的词元被掩蔽并预测）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_replace_mlm_tokens(tokens, valid_pred_positions, num_mlm_preds, vocab):\n",
    "    import random\n",
    "    # 创建 MLM 的输入和标签\n",
    "    mlm_input = [token for token in tokens]\n",
    "    pred_positions_and_labels = []\n",
    "\n",
    "    # 打乱 valid_pred_positions 中的元素顺序\n",
    "    random.shuffle(valid_pred_positions)\n",
    "    for mlm_pred_position in valid_pred_positions:\n",
    "        if len(pred_positions_and_labels) >= num_mlm_preds:\n",
    "            break\n",
    "        masked_token = None\n",
    "        # 80% 的时间，将 token 替换成 <mask>\n",
    "        if random.random() < 0.8:\n",
    "            masked_token = '<mask>'\n",
    "        else:\n",
    "            # 10% 的时间，将 token 替换成一个随机词元\n",
    "            if random.random() < 0.5:\n",
    "                masked_token = random.choice(vocab.idx_to_token)\n",
    "            # 10% 的时间，保持 token 不变\n",
    "            else:\n",
    "                masked_token = tokens[mlm_pred_position]\n",
    "        # 完成替换\n",
    "        mlm_input[mlm_pred_position] = masked_token\n",
    "        # 记录下被替换的词元位置，及其被替换前的词元，作为标签\n",
    "        pred_positions_and_labels.append((mlm_pred_position, tokens[mlm_pred_position]))\n",
    "    return mlm_input, pred_positions_and_labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数 `get_mlm_data_from_tokens` 通过调用 `get_replace_mlm_tokens` 将 BERT 输入序列作为输入，然后**返回三个元素**：\n",
    "* 被掩蔽替换后输入序列的词元索引 `vocab[mlm_input]`\n",
    "* 要预测的词元的位置 `pred_positions`\n",
    "* 要预测的词元的标签词元索引 `vocab[mlm_pred_labels]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mlm_data_from_tokens(tokens, vocab):\n",
    "    valid_pred_positions = [] # 初始化可以被掩蔽词元的位置列表\n",
    "    for i,token in enumerate(tokens):\n",
    "        # 排除掉特殊词元\n",
    "        if token in ['<cls>', '<sep>']:\n",
    "            continue\n",
    "        valid_pred_positions.append(i)\n",
    "    \n",
    "    # 选择 15% 的词元进行掩蔽\n",
    "    num_mlm_preds = max(1, round(len(tokens) * 0.15))\n",
    "    mlm_input, pred_positions_and_labels = get_replace_mlm_tokens(tokens, valid_pred_positions, num_mlm_preds, vocab)\n",
    "    # 根据被掩蔽词元的位置进行排序\n",
    "    pred_positions_and_labels = sorted(pred_positions_and_labels, key=lambda x: x[0])\n",
    "    # 分别取出被掩蔽词元的位置和标签\n",
    "    pred_positions = [v[0] for v in pred_positions_and_labels]\n",
    "    mlm_pred_labels = [v[1] for v in pred_positions_and_labels]\n",
    "\n",
    "    return vocab[mlm_input], pred_positions, vocab[mlm_pred_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [sent for paragraph in paragraphs for sent in paragraph]\n",
    "vocab = utils.Vocab(sentence, min_freq=5)\n",
    "# 添加三个特殊词元 <cls>, <sep>, <mask>\n",
    "special_tokens = ['<mask>', '<cls>', '<sep>']\n",
    "for token in special_tokens:\n",
    "    vocab.idx_to_token.append(token)\n",
    "    vocab.token_to_idx[token] = len(vocab.idx_to_token) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlm_input masked: [20255, 20255, 2564, 20255, 20255, 20255, 20255, 20255]\n",
      "masked words: [b'(', b'commonly', b'b', b'on', b'1996', b'county', b'of', b'of']\n"
     ]
    }
   ],
   "source": [
    "mlm_input, pred_pos, mlm_labels = get_mlm_data_from_tokens(tokens=nsp_data[0][0], vocab=vocab)\n",
    "print(\"mlm_input masked:\", tf.gather(mlm_input, pred_pos).numpy().tolist())\n",
    "print(\"masked words:\", tf.gather(vocab.idx_to_token, mlm_labels).numpy().tolist())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(3) 合并两个任务的数据**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了将两个任务的数据合并，我们还需要定义一个辅助函数 `pad_BERT_inputs`，通过填充，我们可以**将所有的句子处理到相同的长度，从而将不同的句子组装为批量数据**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_BERT_inputs(examples, max_len, vocab):\n",
    "    # examples 是一个列表，其中的每个元素都包含 5 个元素\n",
    "    # 由函数 get_nsp_data_from_paragraph 和 get_mlm_data_from_tokens 生成\n",
    "    # 分别是：序列词元索引、被掩蔽词元的位置、被掩蔽词元的标签索引、段落标记、下一句预测标签\n",
    "    max_num_mlm_preds = round(max_len * 0.15) # 最多被掩蔽词元的数量\n",
    "    # 初始化 BERT 所需要的各个输入和标签\n",
    "    all_token_ids, all_segments, valid_lens  = [], [], []\n",
    "    all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []\n",
    "    nsp_labels = []\n",
    "    \n",
    "    for (token_ids, pred_positions, mlm_pred_label_ids, segments, is_next) in examples:\n",
    "        # 在末尾添加 <pad> 符号，填充句子到 max_len 长度\n",
    "        all_token_ids.append(tf.constant(token_ids + [vocab['<pad>']] * (max_len - len(token_ids)), dtype=tf.int32))\n",
    "        all_segments.append(tf.constant(segments + [1] * (max_len - len(segments)), dtype=tf.int32))\n",
    "        # valid_lens 统计除去 <pad> 之外的有效词元的长度\n",
    "        valid_lens.append(tf.constant(len(token_ids), dtype=tf.float32))\n",
    "\n",
    "        # 预测位置填充到 max_num_mlm_preds 长度\n",
    "        all_pred_positions.append(tf.constant(pred_positions + [0] * (max_num_mlm_preds - len(pred_positions)), dtype=tf.int32))\n",
    "        # weights 和 valid_lens 作用类似，MLM 任务中填充词乘以 weights 从而过滤损失\n",
    "        all_mlm_weights.append(tf.constant([1.0] * len(mlm_pred_label_ids) + \n",
    "                                           [0.0] * (max_num_mlm_preds - len(pred_positions)), dtype=tf.float32))\n",
    "        all_mlm_labels.append(tf.constant(mlm_pred_label_ids + [vocab[\"<pad>\"]] * (max_num_mlm_preds - len(mlm_pred_label_ids)), dtype=tf.int32))\n",
    "        # is_next 作为 NSP 的标签\n",
    "        nsp_labels.append(tf.constant(is_next, dtype=tf.int32))\n",
    "    \n",
    "    return (all_token_ids, all_segments, valid_lens, \n",
    "            all_pred_positions, all_mlm_weights, all_mlm_labels, nsp_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`WikiTextDataset` 类在初始化函数 `__init__` 中准备好 `pad_BERT_inputs` 所需要的 `examples`，然后保存 BERT 所需的所有数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiTextDataset:\n",
    "    def __init__(self, path : str, max_len : int=100, batch_size : int=32) -> None:\n",
    "        self.max_len = max_len\n",
    "        self.batch_size = batch_size\n",
    "        paragraphs = read_wikitext(path) # 读取数据集\n",
    "        # 进行分词，按单词进行分词\n",
    "        paragraphs = [utils.english_tokenize(paragraph,token=\"word\") for paragraph in paragraphs]\n",
    "        # 将段落拆分为句子\n",
    "        sentences = [sentence for paragraph in paragraphs for sentence in paragraph]\n",
    "\n",
    "        # 创建词表\n",
    "        self.vocab = utils.Vocab(sentences, min_freq=5)\n",
    "        # 为词表加入三个特殊词元\n",
    "        special_tokens = ['<mask>', '<cls>', '<sep>']\n",
    "        for token in special_tokens:\n",
    "            self.vocab.idx_to_token.append(token)\n",
    "            self.vocab.token_to_idx[token] = len(self.vocab.idx_to_token) - 1\n",
    "        \n",
    "        # 获得 NSP 任务的数据\n",
    "        examples = []\n",
    "        for paragraph in paragraphs:\n",
    "            examples.extend(get_nsp_data_from_paragraph(paragraph, paragraphs, max_len))\n",
    "        # 获得 MLM 任务的数据\n",
    "        examples = [(get_mlm_data_from_tokens(tokens, self.vocab) + (segments, is_next)) for tokens, segments, is_next in examples]\n",
    "\n",
    "        # 填充数据\n",
    "        (self.all_token_ids, self.all_segments, self.valid_lens, \n",
    "         self.all_pred_positions, self.all_mlm_weights, \n",
    "         self.all_mlm_labels, self.nsp_labels) = pad_BERT_inputs(examples, max_len, self.vocab)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.all_token_ids[idx], self.all_segments[idx], self.valid_lens[idx],\n",
    "                self.all_pred_positions[idx], self.all_mlm_weights[idx],\n",
    "                self.all_mlm_labels[idx], self.nsp_labels[idx])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.all_token_ids)\n",
    "\n",
    "    def create_dataset(self):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((self.all_token_ids, self.all_segments, self.valid_lens,\n",
    "                                                      self.all_pred_positions, self.all_mlm_weights,\n",
    "                                                      self.all_mlm_labels, self.nsp_labels))\n",
    "        dataset = dataset.shuffle(len(self.all_token_ids)).batch(self.batch_size)\n",
    "        return dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们创建数据集实例，加载批量数据，检查函数接口工作是否正常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./source/data/text/wikitext-2/wiki.train.tokens\"\n",
    "wikitext = WikiTextDataset(path=path, max_len=64, batch_size=32)\n",
    "train_iter = wikitext.create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 64)\n",
      "(32, 64)\n",
      "(32,)\n",
      "(32, 10)\n",
      "(32, 10)\n",
      "(32, 10)\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "for (tokens_X, segments_X, valid_lens_X, pred_positions_X, mlm_weights_X, mlm_Y, nsp_Y) in train_iter:\n",
    "    print(tokens_X.shape) # (batch_size, max_len)\n",
    "    print(segments_X.shape) # (batch_size, max_len)\n",
    "    print(valid_lens_X.shape) # (batch_size,)\n",
    "    print(pred_positions_X.shape) # (batch_size, max_num_mlm_preds)\n",
    "    print(mlm_weights_X.shape) # (batch_size, max_num_mlm_preds)\n",
    "    print(mlm_Y.shape) # (batch_size, max_num_mlm_preds)\n",
    "    print(nsp_Y.shape) # (batch_size,)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们可以检查词表大小，在**过滤了频次小于 5 的单词后，词表大小依然达到了两万**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词表大小： 20258\n"
     ]
    }
   ],
   "source": [
    "print(\"词表大小：\", len(wikitext.vocab))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7.3.3 BERT预训练**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原始 BERT 共实现了两个版本：\n",
    "* 基础版：$\\text{BERT}_\\text{BASE}$，`num_layers = 12, num_hiddens = 768, num_heads = 12`，参数量 1.1 亿\n",
    "* 加强版：$\\text{BERT}_\\text{LARGE}$，`num_layers = 24, num_hiddens = 1024, num_heads = 16`，参数量 3.4 亿\n",
    "\n",
    "以上两个 BERT 模型对于演示都太大了，训练速度很慢，我们创建一个非常小的 BERT，指定 `num_layers = 2, num_hiddens = 256, num_heads = 4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-13 15:10:46.842556: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-13 15:10:48.433562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30000 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1b:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "path = \"./source/data/text/wikitext-2/wiki.train.tokens\"\n",
    "wikitext = ch7.WikiTextDataset(path=path, max_len=64, batch_size=512)\n",
    "train_iter = wikitext.create_dataset()\n",
    "vocab = wikitext.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = ch7.BERTModel(vocab_size=len(vocab),num_hiddens=256,num_layers=2,num_heads=4,\n",
    "                     ffn_num_hiddens=128,norm_shape=[2],dropout=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们定义一个函数 `get_BERT_pretrain_loss` 来计算 BERT 预训练时候的损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_BERT_pretrain_loss(model, vocab_size, tokens_X, segments_X, valid_lens_X, pred_positions_X, mlm_weights_X, mlm_Y, nsp_Y):\n",
    "    # 前向计算\n",
    "    _, mlm_Y_hat, nsp_Y_hat = model(tokens_X, segments_X, valid_lens_X, pred_positions_X, training=True)\n",
    "    \n",
    "    # 计算 MLM 损失\n",
    "    unweighted_mlm_l = tf.keras.losses.sparse_categorical_crossentropy(y_true=mlm_Y, y_pred=mlm_Y_hat)\n",
    "    mlm_l = unweighted_mlm_l * mlm_weights_X # 形状: (batch_size, max_num_mlm_preds)\n",
    "    mlm_l = tf.reduce_sum(mlm_l) / (tf.reduce_sum(mlm_weights_X) + 1e-8) # 做数值保护\n",
    "\n",
    "    # 计算 NSP 损失\n",
    "    nsp_l = tf.keras.losses.sparse_categorical_crossentropy(y_true=nsp_Y, y_pred=nsp_Y_hat)\n",
    "    nsp_l = tf.reduce_mean(nsp_l)\n",
    "    # 总体损失\n",
    "    loss = mlm_l + nsp_l\n",
    "\n",
    "    return mlm_l, nsp_l, loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义训练函数 `pretrain_BERT`\n",
    "* 与以往的训练逻辑不同，我们这里**直接控制模型梯度下降更新的迭代次数** `total_steps`，而不使用 `Epochs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_BERT(model, train_iter, vocab, total_steps : int=50, lr : float=0.01):\n",
    "    # 定义优化器\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "\n",
    "    # 展示训练进度\n",
    "    animator = utils.Animator(xlabel='epoch', ylabel=\"loss\", xlim=[1, total_steps], legend=['mlm_loss', 'nsp_loss'])\n",
    "\n",
    "    step = 0\n",
    "    batch_mlm_loss = tf.constant(0.0)\n",
    "    batch_nsp_loss = tf.constant(0.0)\n",
    "    num_batches = tf.constant(0.0)\n",
    "    num_tokens = 0\n",
    "    total_time = 0\n",
    "\n",
    "    while step < total_steps:\n",
    "        for (tokens_X, segments_X, valid_lens_X, pred_positions_X, mlm_weights_X, mlm_Y, nsp_Y) in train_iter:\n",
    "            start = time.time()\n",
    "            with tf.GradientTape() as tape:\n",
    "                mlm_l, nsp_l, loss = get_BERT_pretrain_loss(model, len(vocab), tokens_X, segments_X, valid_lens_X, \n",
    "                                                            pred_positions_X, mlm_weights_X, mlm_Y, nsp_Y)\n",
    "            weights = model.trainable_variables\n",
    "            grads = tape.gradient(loss, weights)\n",
    "            optimizer.apply_gradients(zip(grads, weights))\n",
    "\n",
    "            batch_mlm_loss += mlm_l\n",
    "            batch_nsp_loss += nsp_l\n",
    "            num_batches += 1.0\n",
    "            num_tokens += tokens_X.shape[0]\n",
    "            end = time.time()\n",
    "            total_time += end - start\n",
    "\n",
    "            animator.add(step+1, ((batch_mlm_loss/num_batches).numpy(), (batch_nsp_loss/num_batches).numpy()))\n",
    "            step += 1\n",
    "            if step == total_steps:\n",
    "                break\n",
    "    print(f\"{num_tokens / total_time:.2f} 句子对 / 每秒\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1821.41 句子对 / 每秒\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bert = pretrain_BERT(bert, train_iter, vocab, total_steps=50, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "# bert.save_weights(\"./source/model/ch7_bert_pretrain.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们尝试**利用预训练好的 BERT 模型来做词嵌入，将词元、句子表示为向量**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_BERT_encoding(model, vocab, tokens_a, tokens_b=None):\n",
    "    # 将 tokens_a 和 tokens_b 变换为 BERT 序列\n",
    "    tokens, segments = ch7.get_BERT_tokens_and_segments(tokens_a, tokens_b)\n",
    "    \n",
    "    # 获得词元索引，并添加 batch_size 维度\n",
    "    tokens_ids = tf.constant(vocab[tokens], dtype=tf.int32)[None, :]\n",
    "    segments = tf.constant(segments, dtype=tf.int32)[None, :]\n",
    "    valid_len = tf.constant([len(tokens)], dtype=tf.int32)\n",
    "\n",
    "    # 获得 BERT 编码器的输出\n",
    "    encoded_X, _, _ = model(tokens_ids, segments, valid_len)\n",
    "    return encoded_X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们下面来尝试对一段话进行编码：`what is apple ?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "编码后句子的形状： (1, 6, 256)\n",
      "编码后句子的整体表示 <cls> 的形状： (1, 256)\n",
      "编码后，词元 apple 的向量表示：\n",
      " tf.Tensor([ 0.95089483 -0.8938876   1.120297  ], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tokens_a = ['what', 'is', 'apple', '?']\n",
    "# 添加上 <cls> 和 <sep> 词元\n",
    "# 变为 ['<cls>', 'what', 'is', 'apple', '?', '<sep>']\n",
    "encoded_text = get_BERT_encoding(bert, vocab, tokens_a)\n",
    "encoded_cls = encoded_text[:, 0, :]\n",
    "encoded_text_apple = encoded_text[:, 3, :]\n",
    "print(\"编码后句子的形状：\", encoded_text.shape)\n",
    "print(\"编码后句子的整体表示 <cls> 的形状：\", encoded_cls.shape)\n",
    "print(\"编码后，词元 apple 的向量表示：\\n\", encoded_text_apple[0, :3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们接下来为这句话**添加下文** `tokens_b`，观察词元 `apple` 的**编码结果的变化**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "编码后句子的形状： (1, 12, 256)\n",
      "编码后句子的整体表示 <cls> 的形状： (1, 256)\n",
      "编码后，词元 apple 的向量表示：\n",
      " tf.Tensor([ 1.2560798  -1.0285051   0.98350006], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tokens_a = ['what', 'is', 'apple', '?']\n",
    "tokens_b = ['one', 'of', 'my', 'favorite', 'fruits']\n",
    "# 添加上 <cls> 和 <sep> 词元\n",
    "# 变为 ['<cls>', 'what', 'is', 'apple', '?', '<sep>', 'one', 'of', 'my', 'favorite', 'fruits', '<sep>']\n",
    "encoded_text = get_BERT_encoding(bert, vocab, tokens_a, tokens_b)\n",
    "encoded_cls = encoded_text[:, 0, :]\n",
    "encoded_text_apple = encoded_text[:, 3, :]\n",
    "print(\"编码后句子的形状：\", encoded_text.shape)\n",
    "print(\"编码后句子的整体表示 <cls> 的形状：\", encoded_cls.shape)\n",
    "print(\"编码后，词元 apple 的向量表示：\\n\", encoded_text_apple[0, :3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们可以观察到\n",
    "* **同一个词元在不同的上下文语境中，将会得到不同的词元向量编码表示**\n",
    "* 这验证了 **BERT 是上下文敏感的词元编码器**\n",
    "* 上下文敏感的词元编码能够更加正确地表示词元在语境中的含义"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **练习**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
