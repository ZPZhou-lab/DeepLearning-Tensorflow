{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chap4：循环神经网络RNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-28 19:59:45.249079: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-28 19:59:45.392470: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-28 19:59:45.425278: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-05-28 19:59:46.072669: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/chenguangze/software/miniconda3/lib/:/home/chenguangze/software/miniconda3/lib/:/home/chenguangze/software/miniconda3/envs/tensorflow/lib/\n",
      "2023-05-28 19:59:46.072765: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/chenguangze/software/miniconda3/lib/:/home/chenguangze/software/miniconda3/lib/:/home/chenguangze/software/miniconda3/envs/tensorflow/lib/\n",
      "2023-05-28 19:59:46.072772: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import jieba\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "from source.code import ch4\n",
    "from source.code import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果你要在服务器上和别人共用GPU，可以设置你需要的显存资源\n",
    "utils.gpu_limitation_config(device=0,memory=30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.2 循环神经网络**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2.1 循环神经网络模型**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面我们介绍了截断语言模型和 **k-gram 语法模型**，选择截断长度 $\\tau$，则 $x_t\\sim P(x_t | x_{t-1},\\cdots,x_{t-\\tau})$，如果我们想要将 $t-\\tau$ 之前产生的影响施加给 $x_t$，需要增加 $\\tau$，当使用自回归模型时，**模型参数会随之呈指数增长**，因为词表 $\\mathcal{V}$ 需要存储 $|\\mathcal{V}|^{\\tau}$ 个词元数字"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，不妨考虑**隐状态自回归模型**：\n",
    "$$\n",
    "P(x_t | x_{t-1},\\cdots,x_1) \\approx P(x_t|h_{t-1})\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们用 $h_{t-1}$ 表示**隐状态**（**hidden state**），也称为隐藏变量，它存储了到时间步 $t-1$ 为止的序列信息，通常，我们基于当前输入 $x_t$ 和先前状态 $h_{t-1}$ 来计算时间步 $t$ 的隐状态：\n",
    "$$\n",
    "h_t = f(x_t,h_{t-1})\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里需要明确区分两个概念：前面介绍的**全连接神经网络的隐藏层**，和本节所讨论的**隐藏状态**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1) 无隐藏状态的神经网络**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑单隐藏层的全连接网络，设激活函数为 $\\phi$，给定一个批量 $X\\in\\mathbb{R}^{n\\times p}$，其中 $n$ 表示批量大小，特征维度为 $p$，则隐藏层 $H\\in\\mathbb{R}^{n\\times h}$ 为：\n",
    "$$\n",
    "H = \\phi(XW_{xh} + b_h)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "隐藏层权重 $W_{xh}\\in\\mathbb{R}^{p\\times h}$，偏置 $b\\in\\mathbb{R}^h$，隐藏神经元个数为 $h$，接下来，，将隐藏变量 $H$ 用作输出层的输入，输出层 $O$ 由下式给出：\n",
    "$$\n",
    "O = HW_{ho} + b_o\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，$O\\in\\mathbb{R}^{n\\times o}$ 是输出变量，$W_{ho}\\in\\mathbb{R}^{ho}\\in\\mathbb{R}^{h\\times o}$ 是输出层权重，$b_o\\in\\mathbb{R}^o$ 是偏置项，如果是分类问题，还可以用 $\\text{softmax}(O)$ 获得概率分布"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "整个网络计算自输入到输出顺序进行，不存在”**看不见**“的隐藏状态，**隐藏变量** $H$ **也仅指代输出层之前，计算过程中的中间变量而已**，并且模型也**没有包含序列模型的时间结构**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2) 使用隐藏状态的循环神经网络**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在假设在时间步 $t$ 有输入 $X_t \\in \\mathbb{R}^{n\\times p},t = 1,\\cdots,T$，$n$ 表示批量大小，即 $n$ 个序列样本，$p$ 表示**特征维度**（词嵌入维度），$X_t$ **的每一行对应于批量样本中时间步** $t$ **处的一个词的特征表示**，接下来，我们用 $H_t\\in\\mathbb{R}^{n\\times h}$ 表示时间步 $t$ 处的隐藏状态，与前面的全连接神经网络不同的是：\n",
    "* 我们还**需要保存前一个时间步的隐藏状态** $H_{t-1}$\n",
    "* 引入一个新的权重参数 $W_{hh}\\in\\mathbb{R}^{h\\times h}$\n",
    "\n",
    "通过它们来描述**如何使用当前时间步，前一个时间步的隐藏变量包含的信息**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具体地说，当前时间步隐藏状态 $H_t$ 由当前时间步的输入 $X_t$ 与前一个时间步的隐藏状态 $H_{t-1}$ 一起计算得出：\n",
    "$$\n",
    "H_t = \\phi(X_tW_{xh} + H_{t-1}W_{hh} + b_h)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与全连接网络相比，上式多出了 $H_{t-1}W_{hh}$ 一项，**上式即是对** $h_t = f(x_t,h_{t-1})$ **的一种建模方式**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从相邻时间步的隐藏变量 $H_{t-1},H_t$ 的关系看到，它们**捕获并保留了序列直到当前时间步的历史信息**，这如同**当前时间步下神经网络的状态或记忆**，因此 $H_t$ 被称为隐状态"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于在当前时间步 $t$，**隐状态使用的定义与前一个时间步中使用的定义相同**，因此 $H_t$ 的计算是**循环的**（**recurre**），于是基于循环计算的隐状态神经网络，被人们称为**循环神经网络**（**Recurrent Neural Network**，简称**RNN**），网络中按上式定义的循环计算层称为**循环层**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，对于时间步 $t$ 的输出，可以类似于全连接神经网络，用隐状态 $H_t$ 计算：\n",
    "$$\n",
    "O_t = H_t W_{ho} + b_o\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意循环神经网络的权重包含：\n",
    "* **从输入到隐藏状态的变换** $W_{xh}$\n",
    "* **从上一个隐状态到下一个隐状态的变换** $W_{hh}$\n",
    "* **从隐状态到输出的变换** $W_{ho}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要说明，上述参数不包含时间步下标 $t$，这意味着**即使在不同的时间步，RNN 也总是使用相同的模型参数**，因此，循环神经网络的**参数开销不会随着时间步的增加而增加**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下图展示了 RNN 在三个相邻时间步的计算逻辑，在任意时间步 $t$，隐状态可以被等价视为：\n",
    "* **拼接当前时间步** $t$ **的输入** $X_t$ **和前一时间步** $t-1$ **的隐状态** $H_{t-1}$\n",
    "* 将拼接的结果送入一个带有激活函数，输出维度为 $h$ 的全连接网络，得到 $t$ 时刻的隐状态 $H_t$\n",
    "\n",
    "这可以**通过简单的分块矩阵乘法证明**，在上面展示的计算中，拼接后的全连接层的参数是 $W_{xh},W_{hh}$ 的拼接，**隐藏状态** $H_t$ **会参与当前时间步输出** $O_t$ **和下一隐藏状态** $H_{t+1}$ **的计算**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../source/Chap4/RNN结构.svg\" width=800>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来展示一下上述计算过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-28 19:59:47.964086: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-28 19:59:48.580924: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30000 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-3.4621983   2.3219366   0.42681783  7.0344963 ]\n",
      " [ 0.8753829  -2.0137646  -0.02432786  0.45059916]\n",
      " [-0.2517208  -1.4720496   3.1036294  -0.7099814 ]\n",
      " [-0.49829063  0.20061529  1.7727383   5.1443534 ]], shape=(4, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "X, W_xh = tf.random.normal(shape=(4,2)), tf.random.normal(shape=(2, 4))\n",
    "H, W_hh = tf.random.normal(shape=(4,4)), tf.random.normal(shape=(4, 4))\n",
    "# 计算下一个隐状态\n",
    "H_next = X @ W_xh + H @ W_hh\n",
    "print(H_next)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们沿着特征轴将 $X_t$，$H_{t-1}$ 拼接，同时将参数 $W_{xh},W_{hh}$ 拼接，重新计算，会发现**结果完全相同**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-3.4621983   2.3219364   0.4268179   7.0344963 ]\n",
      " [ 0.87538284 -2.0137646  -0.02432787  0.4505992 ]\n",
      " [-0.2517209  -1.4720495   3.1036294  -0.7099815 ]\n",
      " [-0.4982906   0.20061529  1.7727385   5.1443534 ]], shape=(4, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 通过拼接重新计算下一个隐状态\n",
    "H_next = tf.concat([X, H], axis=1) @ tf.concat([W_xh, W_hh], axis=0)\n",
    "print(H_next)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面考虑将循环神经网络应用到语言模型，模型根据过去的和当前的词元预测下一个词元，因此我们**将原始序列移位一个词元作为标签**，假设批量大小为 1，即只有一个样本，其文本序列为\"machine\"，前面说过，为了简化模型复杂度，我们演示的是**字符级语言模型**，将本文**词元化为字符而不是单词**，下图演示了**字符级语言建模的循环神经网络计算过程**，我们使用当前和先前的字符预测下一个字符"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../source/Chap4/字符级RNN语言建模.svg\" width=700>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练过程中，我们**对每个时间步的输出层的输出进行softmax操作**，获取预测为每个词元的概率分布 $P(o_t | h_t)$，然后就可以利用**交叉熵损失函数**计算模型预测和标签之间的损失了"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2.2 困惑度 Perplexity**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何评价一个语言模型生成的质量呢，假如给定历史序列 ”it is raining ...“ 让模型开始续写，三个模型给出三种答案：\n",
    "* **模型一**：\"it is raining outside\"\n",
    "  * 模型一最合乎情理，不但补充出正确完整的单词，并且逻辑通顺\n",
    "* **模型二**：\"it is raining banana tree\"\n",
    "  * 模型二糟糕很多，因为产生了一个无意义的续写，尽管如此，它已经学会如何拼写单词，续写的单词正确\n",
    "* **模型三**：\"it is raining piouwkcjda adoiut\"\n",
    "  * 模型三是训练不足，或训练有误的模型，因为它甚至无法给出正确的单词"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一种评估方法是计算序列的似然概率来度量模型的质量，但这会造成一些问题：\n",
    "* 似然概率依据序列长度变化范围很大，其**数值难以理解**（例如对比分类问题的正确率）\n",
    "* 较短的序列比较长的序列容易出现，因此二者似然概率的值差异很大，会**导致结果的比较不太公平**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "信息论中的**比特**能够帮助我们定义合理的度量\n",
    "* 如果想要**压缩文本**，我们可以根据当前词元集预测的下一个词元\n",
    "* **一个更好的语言模型应该能让我们更准确地预测下一个词元**，因此，它应该允许我们在**压缩序列时花费更少的比特**\n",
    "* 我们可以通过序列中所有 $T$ 个词元的交叉熵损失的均值来衡量：\n",
    "$$\n",
    "\\frac{1}{T} \\sum_{t=1}^{T} -\\log{ P(x_t | x_{t-1},\\cdots,x_1) }\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中概率 $P$ 由**训练得到的语言模型给出**，$x_t$ 是时间步 $t$ 处序列中的实际词元，上式**让模型在不同长度的文本上的性能具有可比性**，由于历史原因，NLP科学家更喜欢用**困惑度**（**Perplexity**）来衡量，它是上式的指数：\n",
    "$$\n",
    "\\exp\\left( -\\frac{1}{T} \\sum_{t=1}^{T} \\log{ P(x_t | x_{t-1},\\cdots,x_1) } \\right)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "困惑度可以理解是“**下一个词元的实际选择数的调和平均数**”：\n",
    "* **最好**情况下，模型总是完美预测，**得到标签词元的概率为 1**，此时模型的**困惑度为 1**\n",
    "* **最坏**情况下，模型**得到标签词元的概率为 0**，此时模型的**困惑度为 正无穷**\n",
    "* **基准**情况是，模型**对所有次元的概率输出是均匀分布**，假设词表大小为 $|\\mathcal{V}|$，则**困惑度就等于词表大小** $|\\mathcal{V}|$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据上面几个例子，就能理解**困惑度表示语言模型在给出正确预测时，有多少个无法确定的词元选择**，最优情况就是只有一个选择，困惑度为 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2.3 实现循环神经网络**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1) 从零动手实现**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从零实现有助于我们更清楚了解 RNN 每个计算环节发生了什么"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我们定义 RNN 模型所需要的几个参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(embed_size : int, vocab_size : int, num_hiddens : int):\n",
    "    # 输入维度 p 等于词嵌入维度 embed_size\n",
    "    # 输出维度 o 等于词典大小 vocab_size\n",
    "    num_inputs = embed_size\n",
    "    num_outputs = vocab_size\n",
    "\n",
    "    # 正态分布初始化参数\n",
    "    def normal(shape):\n",
    "        return tf.random.normal(\n",
    "            shape=shape, stddev=0.01, mean=0, dtype=tf.float32)\n",
    "\n",
    "    # 隐藏层参数\n",
    "    W_xh = tf.Variable(normal((num_inputs, num_hiddens)), dtype=tf.float32)\n",
    "    W_hh = tf.Variable(normal((num_hiddens, num_hiddens)), dtype=tf.float32)\n",
    "    b_h = tf.Variable(tf.zeros(num_hiddens), dtype=tf.float32)\n",
    "\n",
    "    # 输出层参数\n",
    "    W_ho = tf.Variable(normal((num_hiddens, num_outputs)), dtype=tf.float32)\n",
    "    b_o = tf.Variable(tf.zeros(num_outputs), dtype=tf.float32)\n",
    "\n",
    "    params = [W_xh, W_hh, b_h, W_ho, b_o]\n",
    "    return params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 RNN 运行的初始化时刻，我们需要使用 $H_1$ 来计算输出 $O_1 = P(O_1|H_1)$，而 $H_1$ 的**计算又依赖于**：\n",
    "$$\n",
    "H_1 = f(X_1,H_0)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，在最开始的时间步 $t=1$，我们**需要有一个初始化的隐藏状态** $H_0$，**才能让计算按照循环的逻辑进行下去**\n",
    "* 我们这里**返回了一个元组** `tuple`，而不仅仅是一个隐藏状态 $H$，是为了统一函数接口，便于**与后续章节中介绍的模型进行比较**\n",
    "* 我们可以通过\n",
    "\n",
    "    ```python\n",
    "    state = init_rnn_state(batch_size, num_hiddens)\n",
    "    H, = state\n",
    "    ```\n",
    "    来获取该 RNN 模型初始时刻的隐藏状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化 RNN 隐藏状态\n",
    "def init_rnn_state(batch_size : int, num_hiddens : int):\n",
    "    H = tf.zeros(shape=(batch_size, num_hiddens))\n",
    "    return (H, )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们就可以定义 RNN 的计算逻辑了，注意**除了返回模型的输出外，还需要返回更新后的隐藏状态，以便下一次循环使用，这是循环神经网络有别于其他模型的最显著特征**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(inputs : tf.Tensor, state : tf.Tensor, params : list):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    inputs : tf.Tensor\n",
    "        输入，形状为 (num_steps, batch_size, embed_size)\n",
    "    state : tf.Tensor\n",
    "        隐藏状态，每个的形状为 (batch_size, num_hiddens)\n",
    "    params : list\n",
    "        RNN 参数列表，包括 [W_xh, W_hh, b_h, W_ho, b_o]\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    outputs : tf.Tensor\n",
    "        输出，形状为 (num_steps * batch_size, vocab_size)\n",
    "    state : tf.Tensor\n",
    "        更新后的隐藏状态，每个的形状为 (batch_size, num_hiddens)\n",
    "    \"\"\"\n",
    "    # 获取参数\n",
    "    W_xh, W_hh, b_h, W_ho, b_o = params\n",
    "    H, = state\n",
    "\n",
    "    outputs = []\n",
    "    # 依次计算每个时间步的输出\n",
    "    # X 的形状为 (batch_size, embed_size)\n",
    "    for X in inputs:\n",
    "        H = tf.tanh(X @ W_xh + H @ W_hh + b_h) # 更新隐藏状态\n",
    "        Y = H @ W_ho + b_o # 输出，形状为 (batch_size, vocab_size)\n",
    "        Y = tf.nn.softmax(Y, axis=1) # 得到概率分布\n",
    "        outputs.append(Y)\n",
    "\n",
    "    # 返回输出，最后一个时间步的隐藏状态\n",
    "    return tf.concat(outputs, axis=0), (H, )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "拥有了上述所有基本函数后，我们可以创建一个类来包装所有的函数，存储模型所需要的参数，并执行推理逻辑\n",
    "* 注意，`__call__` 方法会返回两个变量\n",
    "  * 一个是模型的输出 `output`\n",
    "  * 一个是经过计算，更新后的隐藏状态 `state`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNBlockScratch:\n",
    "    def __init__(self, embed_size, vocab_size, num_hiddens, init_state, rnn_func, get_params):\n",
    "        self.embed_size = embed_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.init_state = init_state\n",
    "        self.forward = rnn_func\n",
    "\n",
    "        # 初始化模型参数\n",
    "        self.trainable_variables = get_params(embed_size, vocab_size, num_hiddens)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embed_size)\n",
    "\n",
    "    def __call__(self, X : tf.Tensor, state : tf.Tensor):\n",
    "        \"\"\"\n",
    "        X : tf.Tensor\n",
    "            输入，形状为 (batch_size, num_steps)\n",
    "        state : tf.Tensor\n",
    "            隐藏状态\n",
    "        \"\"\"\n",
    "        X = tf.transpose(X) # 将形状变为 (num_steps, batch_size)\n",
    "        X = self.embedding(X) # 词嵌入，形状为 (num_steps, batch_size, embed_size)\n",
    "        return self.forward(X, state, self.trainable_variables)\n",
    "\n",
    "    # 获取初始时刻的隐藏状态\n",
    "    def begin_state(self, batch_size : int, *args, **kwargs):\n",
    "        return self.init_state(batch_size, self.num_hiddens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们生成一些随机数据，检查模型是否能够正确计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入形状 (batch_size, num_steps)\n",
    "batch_size, num_steps = 2, 5\n",
    "X = tf.reshape(tf.range(batch_size*num_steps), (batch_size, num_steps))\n",
    "\n",
    "# 嵌入维度 embed_size\n",
    "embed_size = 8\n",
    "# 词表大小 vocab_size\n",
    "vocab_size = 28\n",
    "\n",
    "# 创建模型\n",
    "num_hiddens = 16\n",
    "model = RNNBlockScratch(\n",
    "    embed_size, vocab_size, num_hiddens, \n",
    "    init_state=init_rnn_state, rnn_func=rnn, get_params=get_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始时隐藏状态数量： 1\n",
      "初始时隐藏状态形状： (2, 16)\n",
      "输出形状： (10, 28)\n",
      "隐藏状态数量： 1\n",
      "隐藏状态形状： (2, 16)\n"
     ]
    }
   ],
   "source": [
    "# 获取初始隐藏状态\n",
    "state = model.begin_state(X.shape[0])\n",
    "print(\"初始时隐藏状态数量：\", len(state))\n",
    "print(\"初始时隐藏状态形状：\", state[0].shape)\n",
    "\n",
    "Y, new_state = model(X, state)\n",
    "\n",
    "# 查看输出形状\n",
    "print(\"输出形状：\", Y.shape)\n",
    "print(\"隐藏状态数量：\", len(new_state))\n",
    "print(\"隐藏状态形状：\", new_state[0].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面可以看待：\n",
    "* **输出的形状是** `(num_steps * batch_size, vocab_size)`\n",
    "* 隐藏状态的形状保持不变，即 `(batch_size, num_hiddens)`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们来考虑**模型的预测问题**：\n",
    "* 模型需要接收先前的文本序列 `prefix`，然后生成 `prefix` 之后的字符\n",
    "* 在循环遍历prefix中的开始字符时，我们**不断地将隐状态传递到下一个时间步，但是不生成任何输出**，该阶段称为**预热**（**Warm Up**），目的在于让模型自我更新（**更新隐藏状态**），**让隐藏状态捕获历史序列的信息**\n",
    "* 预热结束后，**隐状态的值通常比刚开始的初始值更适合预测**，从而**预测字符并输出**它们"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chinese_text_predict(prefix, num_preds, model, vocab, token : str=\"char\"):\n",
    "    \"\"\"\n",
    "    ### 通过前缀 prefix 来预测后续 num_preds 个字符\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prefix : str\n",
    "        前缀字符，即历史文本序列\n",
    "    num_preds : int\n",
    "        预测字符个数\n",
    "    model : Any\n",
    "        训练好的模型\n",
    "    vocab : Vocab\n",
    "        词表\n",
    "    \"\"\"\n",
    "    # 如果词元是单词，先对prefix进行分词\n",
    "    if token == \"word\":\n",
    "        prefix = list(jieba.cut(prefix))\n",
    "\n",
    "    state = model.begin_state(batch_size=1,dtype=tf.float32)\n",
    "    # 初始化输出为前缀的第一个字符\n",
    "    output = [vocab[prefix[0]]]\n",
    "\n",
    "    # 对输入做变换的预处理函数\n",
    "    # 获取 output 的最后一个字符，作为下一次预测的输入\n",
    "    # 相当于获取 X_t\n",
    "    def get_input():\n",
    "        return tf.reshape(tf.constant([output[-1]]), (1, 1))\n",
    "\n",
    "    # 预热期，先将前缀字符的输出作为输入\n",
    "    for y in prefix[1:]:\n",
    "        # 不做预测，只更新隐藏状态\n",
    "        _, state = model(get_input(), state)\n",
    "        output.append(vocab[y])\n",
    "    \n",
    "    # 预测期，将前一时间步的输出作为当前时间步的输入\n",
    "    # 让模型开始滚动预测\n",
    "    for _ in range(num_preds):\n",
    "        y, state = model(get_input(), state)\n",
    "        output.append(int(y.numpy().argmax(axis=1).reshape(1)))\n",
    "    \n",
    "    # 将输出索引转换为字符串\n",
    "    output_str = ''.join([vocab.idx_to_token[i] for i in output])\n",
    "    return output_str"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们导入我们的词表，测试函数工作是否正常，来看看未经过训练的模型会生成什么内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:01<00:00, 5091.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# 导入词表\n",
    "data_loeader = ch4.SeqDataLoader(\n",
    "    file=\"../source/data/text/wiki_zh_2019/wiki_zh_2019.txt\",token=\"char\")\n",
    "vocab = data_loeader.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建模型\n",
    "model = ch4.RNNBlockScratch(\n",
    "    embed_size=64, vocab_size=len(vocab), num_hiddens=256,\n",
    "    init_state=init_rnn_state, rnn_func=rnn, get_params=get_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'今天天气真好，我想破辽友聆勒籴爸语惘涵'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch4.chinese_text_predict(\"今天天气真好，我想\", 10, model, vocab)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于我们**还没有训练网络，它会生成荒谬胡乱的预测结果**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2) 使用 API 实现**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Tensorflow` 的高级API提供了循环神经网络的实现，我们构造一个具有**256个隐藏单元**的**单隐藏层**的循环神经网络层rnn_layer\n",
    "* 事实上，目前我们还没有讨论 **RNN 中多隐藏层层循环神经网络的意义**，这会在后续章节介绍\n",
    "* 当层数为 1 时，计算逻辑与我们上面介绍的相同\n",
    "* 在 `Tensorflow` 中，所有的 RNN 模型都需要用 `tf.keras.layers.RNN` 包装\n",
    "  * `time_major` 参数，确定输出维度排列是 `(num_steps, batch_size, output_size)`，\n",
    "    * 否则，批量会成为第一个维度，即输出形状为 `(batch_size, num_steps, output_size)`\n",
    "  * `return_sequences` 控制是否返回输出序列\n",
    "  * `return_state` 控制是否返回隐藏状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hiddens = 256\n",
    "# 先定义 RNN 块，再定义 RNN 层\n",
    "rnn_cell = tf.keras.layers.SimpleRNNCell(num_hiddens,kernel_initializer='glorot_uniform')\n",
    "rnn_layer = tf.keras.layers.RNN(\n",
    "    rnn_cell, time_major=True, return_sequences=True, return_state=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 256])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "state = rnn_cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)\n",
    "state.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成随机的输入数据测试模型的输出结果\n",
    "* 特别注意，`rnn_layer` 的**输出不涉及我们文本预测任务输出层的计算**，它**返回的是每个时间步的隐藏状态**\n",
    "  * 这些**隐藏状态可以用来作为后续输出层的输入**，就像我们上面设计的 $O_t = H_tW_{ho} + b_o$ 一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出形状： (10, 32, 256)\n",
      "隐藏状态形状： (32, 256)\n"
     ]
    }
   ],
   "source": [
    "num_steps, vocab_size = 10, 28\n",
    "X = tf.random.uniform(shape=(num_steps, batch_size, vocab_size))\n",
    "Y, state_new = rnn_layer(X, state)\n",
    "\n",
    "# 打印输出形状\n",
    "print(\"输出形状：\", Y.shape)\n",
    "print(\"隐藏状态形状：\", state_new.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，`rnn_layer` 输出**最后一个时间步就是模型最后更新得到的隐藏状态**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_timestep_output = Y[-1]\n",
    "# 判断最后一个时间步的输出和隐藏状态是否相等\n",
    "np.all(last_timestep_output.numpy() == state_new.numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们可以继承 `tf.keras.layers.Layer`，定义 RNN 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(tf.keras.layers.Layer):\n",
    "    def __init__(self, rnn_layer, embed_size : int, vocab_size : int, \n",
    "                 trainable=True, name=None, dtype=None, dynamic=False, **kwargs):\n",
    "        super().__init__(trainable, name, dtype, dynamic, **kwargs)\n",
    "        self.rnn = rnn_layer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        # 定义嵌入层\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embed_size)\n",
    "        # 定义输出层\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "        self.softmax = tf.keras.layers.Softmax()\n",
    "    \n",
    "    def call(self, inputs : tf.Tensor, state : tf.Tensor, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        inputs : tf.Tensor\n",
    "            输入，形状为 (batch_size, num_steps)\n",
    "        \"\"\"\n",
    "        # 交换输入的时间步和批量维度，然后进行词嵌入\n",
    "        X = self.embedding(tf.transpose(inputs))\n",
    "        \n",
    "        # RNN 会返回两个输出 Y 和 state\n",
    "        # *state 会将 RNN 返回的 state 包装为一个列表\n",
    "        # 这让函数的返回与我们之前接口保持统一\n",
    "        Y, *state = self.rnn(X, state) # 输出 Y 形状为 (num_steps, batch_size, num_hiddens)\n",
    "\n",
    "        # 将输出形状变换为 (num_steps * batch_size, num_hiddens)\n",
    "        Y = tf.reshape(Y, (-1, Y.shape[-1]))\n",
    "        \n",
    "        # 计算输出\n",
    "        output = self.dense(Y) # 形状为 (num_steps * batch_size, vocab_size)\n",
    "        output = self.softmax(output)\n",
    "        \n",
    "        return output, state\n",
    "\n",
    "    # 获取初始隐藏状态\n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.rnn.cell.get_initial_state(*args, **kwargs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们对上面的 `*state` 运算做简单说明，在刚才的例子中，如果我们**更换语句**：\n",
    "```python\n",
    "Y, state_new = rnn_layer(X, state)\n",
    "```\n",
    "为：\n",
    "```python\n",
    "Y, *state_new = rnn_layer(X, state)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出形状： (10, 32, 256)\n",
      "隐藏状态类型： <class 'list'>\n",
      "隐藏状态数量： 1\n",
      "隐藏状态形状： (32, 256)\n"
     ]
    }
   ],
   "source": [
    "Y, *state_new = rnn_layer(X, state)\n",
    "\n",
    "# 打印输出形状\n",
    "print(\"输出形状：\", Y.shape)\n",
    "print(\"隐藏状态类型：\", type(state_new))\n",
    "print(\"隐藏状态数量：\", len(state_new))\n",
    "print(\"隐藏状态形状：\", state_new[0].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对比原本的输出，此时 `state_new` 被包装到了一个列表中，这就与我们在**从零开始实现**时定义的接口相同了\n",
    "* 我们可以测试一下模型的接口是否工作正常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn_model(num_hiddens : int, embed_size : int, vocab_size : int):\n",
    "    \"\"\"\n",
    "    创建 RNN 模型\n",
    "    \"\"\"\n",
    "    rnn_cell = tf.keras.layers.SimpleRNNCell(num_hiddens,kernel_initializer=\"glorot_uniform\")\n",
    "    rnn_layer = tf.keras.layers.RNN(\n",
    "        rnn_cell, time_major=True, return_sequences=True, return_state=True)\n",
    "    model = RNNModel(rnn_layer, embed_size, vocab_size)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建模型\n",
    "model = ch4.create_rnn_model(num_hiddens=256, embed_size=64, vocab_size=len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'今天天气真好，我想墡W己菻胱奠冒℃龛ो'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch4.chinese_text_predict(\"今天天气真好，我想\", 10, model, vocab)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2.4 模型训练**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练 RNN 时，我们可能会遇到**因矩阵乘法导致的数值不稳定问题**：\n",
    "* 对于长度为 $T$ 的序列，我们在迭代中计算这 $T$ 个时间步上的梯度\n",
    "  $$\n",
    "  Y_T = f(H_T) = f(g(X_T,H_{T-1})) = f(g(X_T,g(X_{T-1},H_{T-2}))) = \\cdots\n",
    "  $$\n",
    "* 根据**链式法则**，在**反向传播过程中产生长度为** $\\mathcal{O}(T)$ **的矩阵乘法**\n",
    "* 在时间长度 $T$ 较大时，使用该梯度更新参数会**导致数值不稳定**，梯度消失和梯度爆炸都有可能出现"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，循环神经网络模型往往**需要额外的方式来支持稳定训练**，在梯度下降的更新过程中，假设一次迭代我们将 $\\boldsymbol{\\theta}$ 更新为 $\\boldsymbol{\\theta} - \\eta \\boldsymbol{g}$，如果进一步假设优化函数 $f$ 性质良好，例如**Lipchitz连续**，则：\n",
    "$$\n",
    "|f(\\boldsymbol{\\theta}) - f(\\boldsymbol{\\theta} - \\eta\\boldsymbol{g}) | \\leq L\\eta \\|\\boldsymbol{g}\\|\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这意味着我们**不会观察到超过** $L\\eta\\|\\boldsymbol{g}\\|$ **的变化**，这既是坏事，也是好事：\n",
    "* 坏事在于**它限制了模型更新的速度**\n",
    "* 好处在于**它减缓了模型变糟的程度**，尤其是当模型朝着错误的方向前进时"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有时**梯度可能很大，从而优化算法可能无法收敛**：\n",
    "* 可以通过**降低学习率**来解决这个问题，但如果 $\\eta \\to 0$，在**通常大部分情况下，又会导致模型更新缓慢**，这种改进显然不合理\n",
    "* 另一种主流的方法是**限制梯度的范数**，将 $\\boldsymbol{g}$ 投影到给定半径的球中，进行**梯度裁剪**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定**超球体的半径** $r$，**梯度裁剪执行如下操作**：\n",
    "$$\n",
    "\\boldsymbol{g}\\leftarrow \\min\\left( 1, \\frac{r}{\\|\\boldsymbol{g} \\|} \\right)\\boldsymbol{g}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 经过变换，**梯度范数永远不会超过** $r$\n",
    "* 更新后的梯度完全与 $\\boldsymbol{g}$ **保持在同一方向**\n",
    "* 由于我们使用的是**批量随机梯度下降**，梯度裁剪还**限制了任何小批量数据对参数向量的影响**，因此模型训练不会一个随机的糟糕的批量而崩溃，增强了稳定性"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**梯度裁剪提供了一个快速修复梯度爆炸的方法**，虽然它并不能完全解决问题，但它是众多有效的技术之一"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(grads : list, radius : float):\n",
    "    \"\"\"\n",
    "    ### 梯度裁剪\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    grads : list\n",
    "        每组参数的梯度组成的列表\n",
    "    radius : float\n",
    "        裁剪半径\n",
    "    \"\"\"\n",
    "    radius = tf.constant(radius,dtype=tf.float32)\n",
    "    new_grads = []\n",
    "\n",
    "    # 依次取出每组参数的梯度，将梯度转换为张量\n",
    "    for grad in grads:\n",
    "        if isinstance(grad, tf.IndexedSlices):\n",
    "            new_grads.append(tf.convert_to_tensor(grad))\n",
    "        else:\n",
    "            new_grads.append(grad)\n",
    "    \n",
    "    # 计算梯度范数\n",
    "    norm = tf.math.sqrt(sum([(tf.reduce_sum(grad ** 2)).numpy() for grad in new_grads]))\n",
    "    norm = tf.cast(norm, dtype=tf.float32)\n",
    "\n",
    "    # 范数超过半径，进行裁剪\n",
    "    if tf.greater(norm, radius):\n",
    "        for i, grad in enumerate(new_grads):\n",
    "            new_grads[i] = grad * radius / norm\n",
    "    \n",
    "    return new_grads"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们就可以开始定义模型的训练函数了，它与我们之前章节定义的训练函数有四个不同之处：\n",
    "* 序列数据的不同采样方法（**随机采样策略**和**顺序分区策略**）将导致**隐状态初始化的差异**\n",
    "* 我们在**更新模型参数之前裁剪梯度**\n",
    "* 我们用**困惑度** Perplexity 来评价模型\n",
    "* 我们**在训练过程中就统计每次迭代损失函数的值**，以便于最后计算困惑度"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还在训练过程中**记录了模型处理文本词元的速度**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_text_generation(model, train_iter : ch4.SeqDataLoader, use_random_iter : bool=True, \n",
    "                          Epochs : int=10, lr : float=0.1, verbose : int=1):\n",
    "    # 设定优化器和损失函数\n",
    "    # SparseCategoricalCrossentropy 不需要将标签转换为 one-hot 向量\n",
    "    loss_func = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "\n",
    "    animator = utils.Animator(xlabel='epoch', ylabel='perplexity', \n",
    "                              legend=(\"perplexity\",), xlim=[1, Epochs])\n",
    "\n",
    "    # 存储每个迭代周期的损失和样本量\n",
    "    loss_batch, samples_batch = 0, 0\n",
    "    # 记录单词处理速度\n",
    "    speeds = []\n",
    "\n",
    "    for epoch in range(Epochs):\n",
    "        state, start = None, time.time()\n",
    "        for x_batch, y_batch in train_iter:\n",
    "            # 如果是随机采样，每次迭代都随机初始化隐藏状态\n",
    "            if state is None or use_random_iter:\n",
    "                # 初始化隐藏状态\n",
    "                state = model.begin_state(batch_size=x_batch.shape[0], dtype=tf.float32)\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                y_hat, state = model(x_batch, state)\n",
    "                y = tf.reshape(tf.transpose(y_batch), (-1,))\n",
    "                loss = loss_func(y, y_hat)\n",
    "            weights = model.trainable_variables\n",
    "            grads = tape.gradient(loss, weights)\n",
    "            grads = grad_clipping(grads, 1)\n",
    "            optimizer.apply_gradients(zip(grads, weights))\n",
    "\n",
    "            # 将该批量的损失函数值加到总损失函数值上\n",
    "            loss_batch += loss.numpy() * tf.size(y).numpy()\n",
    "            samples_batch += tf.size(y).numpy()\n",
    "        \n",
    "        end = time.time()\n",
    "        speeds.append(samples_batch / (end - start))\n",
    "        \n",
    "        if epoch == 0 or (epoch + 1) % verbose == 0:\n",
    "            # 计算困惑度\n",
    "            ppl = tf.math.exp(loss_batch / samples_batch).numpy()\n",
    "            animator.add(epoch + 1, [ppl])\n",
    "\n",
    "    print(f\"平均 {np.mean(speeds):.1f} 词元/秒\")\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用余华的几个作品集作为文本训练集做演示，导入**词表和训练数据集**，开始训练模型，我们**设置每个子序列的文本长度为 32**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31692/31692 [00:00<00:00, 303898.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "语料库大小： 864985\n"
     ]
    }
   ],
   "source": [
    "# 导入词表迭代器\n",
    "train_iter = ch4.SeqDataLoader(\n",
    "    file=\"../source/data/text/余华作品集.txt\",token=\"char\", concat=True,\n",
    "    num_lines=50000, batch_size=32, num_steps=32, use_random_iter=True\n",
    ")\n",
    "vocab = train_iter.vocab\n",
    "\n",
    "# 计算语料库大小\n",
    "if isinstance(train_iter.corpus[0], list):\n",
    "    corpus_size = sum(len(c) for c in train_iter.corpus)\n",
    "else:\n",
    "    corpus_size = len(train_iter.corpus)\n",
    "print(\"语料库大小：\", corpus_size)\n",
    "\n",
    "# 实例化模型\n",
    "model = ch4.create_rnn_model(num_hiddens=512, embed_size=64, vocab_size=len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天天气真好，我想基负崇妇艺舞构（瞬趣\n",
      "太阳从西边出来了，条尾凉冥澡裆肚愈停咐\n"
     ]
    }
   ],
   "source": [
    "prefix = [\n",
    "    \"今天天气真好，我想\",\n",
    "    \"太阳从西边出来了，\"\n",
    "]\n",
    "for p in prefix:\n",
    "    print(ch4.chinese_text_predict(p, 10, model, vocab))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "未训练的模型会进行胡乱预测，甚至**无法给出正确的词汇的语句表达**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均 50159.3 词元/秒\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = ch4.train_text_generation(model,train_iter=train_iter,use_random_iter=True,Epochs=10,verbose=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们来测试一下**训练后模型的预测效果**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天天气真好，我想到我们，他们的一个，\n",
      "太阳从西边出来了，他们的一个，他们的一\n"
     ]
    }
   ],
   "source": [
    "prefix = [\n",
    "    \"今天天气真好，我想\",\n",
    "    \"太阳从西边出来了，\"\n",
    "]\n",
    "for p in prefix:\n",
    "    print(ch4.chinese_text_predict(p, 10, model, vocab))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到效果并不好，**中文词元众多，且词元组成更复杂多样，仅依靠这个简单的 RNN 模型实现正确预测非常困难**，模型的困惑度很高\n",
    "* 但相比没有训练过的模型，**模型预测的词汇至少是正确的**\n",
    "* **但可惜词汇单一且重复，不具有任何实际含义**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **练习**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 接下来我们**在英文文本数据集上**测试 RNN 模型\n",
    "   1. 利用小说文本 `../source/data/text/time_traveller.txt`，从 `../source/code/ch4_hw,py` 创建 `TimeTravellerLoader`，请使用**按字母分词**方案，语料库的大小是多少？超参数设计如下：\n",
    "      1. `batch_size = 64` 或者 `batch_size = 128`\n",
    "      2. `num_steps = 32`\n",
    "   2. 针对英文文本，设计文本预测函数 `english_text_predict()`，并将它写入 `../source/code/ch4_hw.py` 以便后续章节使用\n",
    "   3. 创建一个 RNN 模型，设定合适的模型参数 `num_hiddens`, `embed_size`，学习率设定为 `lr = 1`，迭代次数 `Epochs` 设定100以上\n",
    "   4. 给定前缀 `prefix = \"time traveller \"`，测试模型向后预测的结果，你也可以测试其他前缀\n",
    "      1. 模型的输出效果如何？它能正确拼写单词吗？模型的续写存在什么问题？\n",
    "   5. 尝试重新定义训练函数，去除掉**梯度裁剪**，重新创建一个模型并训练，观察训练结果，你会得到什么结论？\n",
    "   6. 如果将词元化方案从**字母词元化**更改为**单词词元化**，重新创建模型并训练，模型的效果如何？\n",
    "      1. **单词词元化的词表大小**是多少？\n",
    "      2. 你觉得用单词词元化训练模型面临的困难有哪些？**如何才能保证比较好的模型效果，降低困惑度**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
